\documentclass[a4paper, 11pt, oneside]{book}
\include{../../Preambles/LectureNotesPreamble}

\begin{document}

\lfoot{\footnotesize Combinatorics 80520}
\begin{titlepage}
    \begin{center}
        \vspace*{1cm}

        \LARGE
        \textbf{The Probabilistic Method in Combinatorics 80721}

        \vspace{0.5cm}
        \Large
        Based on lectures by Dr. Yuval Peled, and the book by Alon and Spencer - \emph{The probabilistic method}

        \vspace{1.5cm}
        \large
        \textbf{Notes by Asaf Etgar}\\
        Spring 2022\\
        

%        \vfill



        \vspace{0.8cm}

        \large
        Last edit:
        \today \\
        For any corrections, requests for additions or notes - please email me at \\
        \ttfamily{asafetgar@gmail.com}\\
        		\vspace{0.8cm}
        \textit{These notes have not been revised by the course staff, and some things may appear differently than in the lectures/ recitations.}

    \end{center}
\end{titlepage}

\tableofcontents
\chapter{Introcuction}
\section{Ramsey Numbers}

\begin{claim}
	For any graph $G = (V,E)$ there exists a partitioning of $V = A\sqcup B$ such that at least half of the edges are $A-B$ edges.
\end{claim}
\begin{proof}
	Consider a random partition of $V$, $A,B$. That is, each vertex $v$ is in $A$ or in $B$ w.p $\frac{1}{2}$ independently. Then:
	\[
	\Exp{e(A,B)} \nim{linearity}= |E|\cdot \prob{e\text{ is an $A,B$ edge}} = \frac{|E|}{2}
	\]
	Which implies that there exists. a partition with said property.
\end{proof}
\begin{remark}
One can prove this claim without the use of probability.	
\end{remark}
There are questions that we do not know yet how to solve without the use of probability:
\begin{yellowBox}
\begin{defn}
	[Ramsey Number] The number $R(k,l)$ is the minimal $n$ such that every graph $G$ over $n$ vertices contains a $k$-clique or an $l$ -anti-clique.
\end{defn}	
\end{yellowBox}
\begin{thm}[Ramsey]\label{thm:Ramsey} $R(k,l)\le {k-l-2\choose k-1}$. In particular, $R(k,k)\le {2k-2\choose k-1} \approx \frac{4^{k-1}}{\sqrt{\pi k}}$
	
\end{thm}
\begin{thm}\label{thm:Lower Bound Ramsey}
	If ${n \choose k}2^{1-{k\choose 2}} < 1$, then $R(k,k) > n$
\end{thm}
\begin{proof}  Consider now a random graph $G\sim G(n,\frac{1}{2})$. For any $A\in {[n]\choose k}$, denote by $M_A$ the event that $A$ is a clique or anti-clique in $G$. Then:
	\[
	\prob{M_A} = \prob{\text{$A$ is a clique}} + \prob{\text{$A$ is an anti-clique}} = 2^{1-{k\choose 2}}
	\]
	And therefore
	
	\[
	\prob{\text{$\exists$ a clique or anti-clique of size $k$}} = \prob{\bigcup_{A\in {[n]\choose k}} M_A }\le \sum_{A\in {[n]\choose k}} \prob{M_A} = {n\choose k} 2^{1-{k\choose 2}} < 1
	\]
	Hence there exists a graph over $n$ vertices without a clique or anti-clique of size $k$.
\end{proof}
\begin{remark}
Note that
	\begin{align*}
		& {n \choose k}2^{1-{k\choose 2}} < 1 \iff {n \choose k} < 2^{{k\choose 2} - 1}
	\end{align*}
	And also, ${n\choose k} \le \frac{n^k}{k!}$, and by Stirling's approximation - $k! \ge \rbk{\frac{k}{e}}^k$. Pluggin in the inequality:
	\[
	{n\choose k} \le \rbk{\frac{en}{k}}^k
	\]
Comparing this to the formula in \ref{thm:Ramsey}, this is a very loose bound (at least for $k,k$).
\end{remark}
\begin{remark}
If $n = 2^{k/2}$, then:
\[ \prob{\text{A random graph contains a clique or anti-clique}} \le {n\choose k}2^{1-{k\choose 2}} \nim{n,k}\To 0 \]

 Which means "almost all graphs are Ramsey graphs", but we do not yet  have any explicit construction.
\end{remark}
This theorem implies that we know the existence of a graph of order $n$ and without clique or anti-clique of size $k\approx 2\log n$. The best construction known without the use of probability is for $k = \log^{C\cdot \log\log\log n}n$.

\chapter{Linearity of Expectation}
\section{Sum-Free Sets}
\begin{thm}
	For any  $B\in {\NN\choose n}$ (with repetitions), there exists $A\in {B\choose n/3}$ such that there are no $a,b,c\in A$ with $a + b = c$
\end{thm}
\begin{proof}
	[Proof (by Erd\H{o}s)] Denote $[x] = x-\Floor{x}$, and for any $t\in [0,1]$ let $A_t = \cbk{b\in B \mid [tb]\in \rbk{\frac13,\frac23}}$. 
		For any $t$, $A_t$ is sum-free: If $a,b\in A_t$ and $[ta],[tb]\in \rbk{\frac13,\frac23}$, then $[a+b]\notin \rbk{\frac13,\frac23}$. We consider the probability space of the coin tosses of $t$. Denote $X_i = \one_{b_i\in A_t}$, then $\prob{X_i=1} = \frac13$. Hence consider the expectation of a size of a random $A_t$:
		\begin{flalign*}
			& \Exp{|A_t|} = \sum_{i\in [n]} \Exp{X_i} = \frac{n}{3}
		\end{flalign*}
\end{proof}
\begin{remark}
The general idea of probabilistic methods is to find an object in which the property always holds, and then average over these objects	
\end{remark}

\section{Tournaments}
\begin{yellowBox}
\begin{defn}
	A tournament is an orientation of $K_n$.
\end{defn}
\begin{defn}
	We say a vertex $v$ \emph{overcomes} some $A\subset V\setminus x$ if $v\to x$ for any $x\in A$ (that is, the orientation of $vx\in E(K_n)$ is $v\to x$).
\end{defn}
\end{yellowBox}

\begin{thm}
	If ${n\choose k}\rbk{1-2^{-k}}^{n-k}$, then there exists a tournament such that for any $A\in {V\choose k}$ there exists $v$ that overcomes $A$.
\end{thm}
\begin{proof}
Denote by $S_k$ the event that for any $A$ of size $k$ there exists an overcoming $v$.
	Consider a random tournament, and let $A\in {V\choose k}$, what is the probability that no $v$ overcomes $A$?
	\[
	\prob{\text{No $v$ overcomes $A$}} = \rbk{1-2^{-k}}^{n-k}
	\]
	(some $v$ overcomes $A$ w.p $2^k$, and they are independent) Then:
	\[
	\prob{S_k^c}\le {n\choose k}\rbk{1-2^{-k}}^{n-k} < 1
	\]
\end{proof}
\begin{remark}
	The union bound is quite similar to linearity of expectation.
\end{remark}
\begin{thm}
	There exists a tournament with at least $n!\cdot 2^{-(n-1)}$ Hamiltonian cycles. 
\end{thm}
\begin{proof}
	Consider a random tournament. Then:
	\begin{flalign*}
		&\Exp{\#\text{ of Hamiltonian cycles}} = \sum_{\pi\in S_n}\prob{\pi(V)\text{is a cycle}} = n!2^{-(n-1)}
	\end{flalign*}
	(the last equation is the probability of this permutation defininig a cycle)
	Then there must exist a tournament with at least this number of cycles.
\end{proof}
\section{??? If you have a suggestion for a name, let me know!}
\begin{thm}
	Let $v_1,\ldots v_n\in \RR^d$ be unit vectors. Then there exists $\varepsilon_1,\ldots\varepsilon_n\in \{\pm 1\}$ such that
	\[
	\norm{\sum_{i\in [n]}\varepsilon_iv_i}\le \sqrt{n}
	\]
	and there exists such $\varepsilon_i$ for the opposite inequality.
\end{thm}
\begin{proof}
	Consider a random choice of $\varepsilon_i$. Denote $X = \norm{\sum_{i\in [n]}\varepsilon_iv_i}^2$. Then:
	\[
	X = \norm{\sum_{i\in [n]}\varepsilon_iv_i^2} = \sum_{i\in [n]} \varepsilon_i^2 v_i + 2\cdot \sum_{i<j} \varepsilon_i\varepsilon_j\tbk{v_i,v_j}
	\]
	then
	\[
	\Exp{X} = n + 2\cdot \sum_{i<j}\tbk{v_i,v_j}\Exp{\varepsilon_i\varepsilon_j} = n
	\]
	Since $\Exp{\varepsilon_i\varepsilon_j} = \Exp{\varepsilon_j}\Exp{\varepsilon_j} = 0\cdot 0 = 0$, and the claim follows as usual.
\end{proof}
\subsection{Derandomization}
We would like to de-randomize the process and find an efficient algorithm of finding these $\varepsilon_i$. By the law of total expectation, $\Exp{X} = \frac12\Exp{X\mid \varepsilon_1 = 1} + \frac12\Exp{X\mid \varepsilon_1 = -1}$.
\begin{claim}
	If we've fixed $\varepsilon_1\ldots \varepsilon_{i-1}$ such that $\Exp{X\mid \varepsilon_1\ldots\varepsilon_{i-1}}\le n$, then we can efficiently find $\varepsilon_i$ such that $\Exp{X\mid \varepsilon_1\ldots\varepsilon_{i-1}, \varepsilon_i}\le n$
\end{claim}
\begin{proof}
	\[
	\Exp{X\mid \varepsilon_1\ldots\varepsilon_{i-1}} \nim{\star}= \frac12\Exp{X\mid \varepsilon_1\ldots\varepsilon_{i-1},\varepsilon_i = 1} + \frac12\Exp{X\mid \varepsilon_1\ldots\varepsilon_{i-1},\varepsilon_i = -1}
	\]
	with $\star$ by law of total expectation (w.r.t the random variable $\varepsilon_i$). But
	\[
	\Exp{X\mid \varepsilon_1\ldots\varepsilon_{i-1},\varepsilon_i = 1} = n + 2\sum_{j'<j \le i}\varepsilon_{j}\varepsilon_{j'}\tbk{v_j,v_i} + 0
	\]
	And we know the values of $\varepsilon_{j'},\varepsilon_j$, so we can compute it efficiently. Then we choose the epsilon Todo{Compete from the book}
\end{proof}
\section{Turan's theorem}
\begin{thm}
	In any graph $(V,E)$, there exists an independent set of size at least $\sum_{v\in V}\frac{1}{deg(v) + 1}$
\end{thm}
\begin{proof}
	Consider a random ordering of $V$. We choose a vertex to add to the set $I$ ("independent") if he appears before all of his neighbors. Clearly $I$ is independent. And:
	\[
	\Exp{|I|} = \sum_{v\in V}\prob{v\in I} = \sum_{v\in V}\prob{\text{$v$ is the first of hie neighbors in the ordering}} = \sum_{v\in V}\frac{1}{deg(v) +1}
	\]
\end{proof}
\begin{cor}
	In $G$ there exists a clique of size $\ge \sum_{v\in V}\frac{1}{n-deg(v)}$
\end{cor}
\begin{thm}
	[Tur\a'an] If the maximal clique is of size $r$, then \[
	r\ge \sum_{v\in V}\frac{1}{n-\deg(v)} \ge \frac{n^2}{n^2-2|E|}
	\]
	Therefore $|E| \le \rbk{1-\frac{1}{n}}\cdot\frac{n^2}{2}$
\end{thm}
\section{Unbalancing Lights}
Let $A$ be an $n\times n$ matrix over $\{\pm 1\}$. There is a switch for every row and every column, which flips all bits corresponding to it.
\begin{thm}
	There exists $x,y\in \cbk{\pm 1}^n$ \footnote{think of $x$ as responsible of rows, and $y$ of columns} such that $x^\top Ay \ge \rbk{\sqrt{\frac{2}{\pi}} + o(1)}n^{\frac32}$
\end{thm}
\begin{proof}
	Choose a random $y$ (that is, $y_i\sim U(\pm 1)$ iid. Let $R_i = \sum_{j=1}^nA^i_jy_j$. Since $y_i$ are iid, $A^i_jy_j \sim $ a sum of $n$ signs$ \pm 1 $ iid. Then by CLT:
	\[
	\frac{1}{\sqrt{n}}R_i\nim{distribution}\To \Nn(0,1)
	\]
	And therefore $\Exp{\frac{1}{\sqrt{n}}|R_i|} \nim{n\to\infty}\To \Exp{|z|} = \sqrt{\frac{2}{\pi}}$. Hence:
	\[
	\Exp{\sum_{i=1}^n |R_i|} = \sum_{i=1}^n \Exp{|R_i|} = \rbk{\sqrt{\frac{2}{\pi}} + o(1)}n^\frac32
	\]
	Then there exists $y$ such that $\sum_{i=1}^n |R_i| \ge \rbk{\sqrt{\frac{2}{\pi}} + o(1)}n^\frac32$. As for $x$, note that 
	\[
	x^\top Ay = \sum_{i=1}^n\sum_{j=1}^n x_iA^i_jy_j = \sum_{i=1}^nx_iR_i \nim{\star}= \sum_{i=1}^n |R_i| \ge \rbk{\sqrt{\frac{2}{\pi}} + o(1)}n^\frac32
	\]
	with $\star$ since we can take $x_i = sign(R_i)$.\footnote{In some sense, this is "the smartest move" in order to get as many light bulbs lit as possible.}
\end{proof}
\section{2-colorings of hypergraphs}
\begin{yellowBox}
\begin{defn}
	[$k$-uniform Hypergraph] $H = (V,E)$  is a $k$-uniform Hypergraph with $V(H)$ its vertices and $E(H) \subset {V(H)\choose k}$. In particular, a $2$-uniform hypergraph is just a graph.
\end{defn}	
\begin{defn}
	[2-coloring of hypergraph] Let $H$ be a $k$-graph. A \emph{2 coloring} of $H$ is a function $f:V(H)\to \{0,1\}$ such that there is no monochromatic edge, that is $\forall e\in E(H) \quad \exists x,y\in e \quad f(x)\neq f(y)$. 
\end{defn}
\begin{defn}
	Denote $m(k)$ the minimal number of edges in a $k$-graph that is not $2$ colorable.
\end{defn}
\end{yellowBox}
\begin{example}
	$m(2) = 3$, consider a triangle.
\end{example}
\begin{example}
	$m(3) = 7$, consider the Fano Plane.
\end{example}
\begin{thm}
\label{thm: bound on minimal edges in no 2 color hyper}
	$m(k)\ge 2^{k-1}$
\end{thm}
\begin{proof}
	Let $H$ be a hypergraph with less than $m2^{k-1}$ edges. Let $f$ be a uniformly random coloring of $H$ Then
	\[
	\prob{e\text{ is monochromatic}} = 2^{1-k}
	\]Therefore
	\[
	\Exp{\#\text{monochromatic edges}} < m\cdot2^{1-k} = 1
	\]
\end{proof}
\begin{thm}
$m(k) = O(k^22^k)$.	
\end{thm}
\begin{proof}
	Let $n = k^2$, and choose $c\cdot k^22^k$ (we specify $c$ later) edges uniformly IID. We show that $\Exp{\#\text{colorings without monochromatic edges}} < 1$. Fix a coloring $\varphi:[n]\to \{0,1\}$. Let $a = |\varphi^{-1}(0)|$, then
	\begin{align*}
	&\prob{\text{$e$ is monochromatic under $\varphi$}} =\\ &\frac{{a\choose k} + {n-a \choose k}}{{n\choose k}} \ge \frac{2\cdot{n/2\choose k}}{{n\choose k}} \ge \frac{2\cdot \frac{(\frac{n}{2} - k)^k}{k!} }{\frac{n^k}{k!}} = 2\rbk{\frac{1}{2}-\frac{k}{n}}^k = 2\cdot\rbk{\frac{1}{2}}^k\rbk{1-\frac{2}{k}}^k \ge c\rbk{\frac{1}{2}}^k
	\end{align*}
	for some $c$. Now we have
	\begin{align*}
		&\Exp{\#\text{colorings without monochromatic edges}} = \sum_{\varphi}\prob{\text{no edge is monochromatic under $\varphi$}} \le \\
		& \le 2^n\rbk{1-\frac{c}{2^k}}^m \le e^{\log(2)\cdot n - c\cdot 2^{-k}\cdot m} \nim{\star}< 1
	\end{align*}
	With $\star$ by choice of $m = \frac{2\log(2)}{c}k^2\cdot 2^k$.
\end{proof}
\begin{thm}
	[Improvement on the bound from \ref{thm: bound on minimal edges in no 2 color hyper}] $m(k)\ge t 2^k\sqrt{\frac{k}{\log(k)}}$.
\end{thm}
\begin{proof}
	Or proof is algorithmic: Let $H$ be with $t$ edges, color all $V(H)$ in blue. Traverse $V(H)$ in a random order, let $v$ the current vertex. If $v$ is the last-visited vertex in a monochromatic blue edge - alter its color to blue. The algorithm fails only if there is a monochromatic red edge. This happens only if the vertex $v = e\cap f$, $f$ is red, $e$ is blue and $v$ is the first in $f$ and last in blue (this is a bad configuration). What is the probability of such configuration to occure? We consider the probability over the coin tosses of $\pi\sim U(S_n)$, and claim
	\[
	\prob{\text{there exists a bad configuration}} < 1
	\]
	We note that:
	\begin{flalign*}
		& \prob{\text{there exists a bad configuration}}\le \Exp{\#\text{$(e,f)$ are bad edges}} \nim{\star}\le m^2\cdot\frac{\rbk{(k-1)!}^2}{(2k-1)!} = \\
		&\frac{m^2}{(2k-1){2k-2 \choose k-1}}\nim{\star\star}= \frac{m^2\cdot(c + o(1))}{\sqrt{k\cdot 4^k}}\nim{?}<1
	\end{flalign*}
	with $\star$ a bound on the number of edges that intersect in a unique vertex, times the probability of having a bad configuration, and $\star\star$ since ${2n\choose n} = \frac{c + o(1)}{\sqrt{n}}2^{2n}$. In order to have $?$, take $m < c'\cdot k^{\frac{1}{4}}\cdot 2^k$. This is not the bound we want - as the power of $k$ is $\frac{1}{4}$. The problem is the expectation sometimes lies - that is, the expectation can be much larger than the probability we want to bound (see the remark below).\\
	
	We traverse the vertices differently: For a vertex $v$, choose $r_v\sim U([0,1])$ i.i.d, and traverse $V(H)$ according to $r_v$ from the smallest to largest. Let $p$ be some probability chosen later, and denote
	\[
	L = \sbk{0,\frac{1-p}{2}}\quad M = \sbk{\frac{1-p}{2},\frac{1+p}{2}}\quad R = \sbk{\frac{1+p}{2},0}
	\]
	Now:
	\begin{flalign*}
		 &\prob{\text{there exists a bad configuration}}\le\\ 
		 &\overbrace{\prob{\exists e\in L\cup R}}^1 + \overbrace{\prob{\exists\text{bad configuration whose intersection is in $M$}}}^2 \le \\
		 &\overbrace{m\cdot 2(|L|)^k}^1 + \overbrace{m^2 \int_{\frac{1-p}{2}}^\frac{1+p}{2}r^{k-1}_v(1-r_v)^{k-1}dr_v}^2= \\
		 & m\cdot rbk{\frac{1-p}{2}}^k + m^2 \int_{\frac{1-p}{2}}^\frac{1+p}{2}r^{k-1}_v(1-r_v)^{k-1}dr_v \le \\
		 & 2m\frac{e^{-pk}}{2^k} + m^2\cdot p\rbk{\frac{1}{4}}^{k-1} \nim{?}<1
	\end{flalign*}
	Choosing $p = \frac{\log k}{k}$ and $m<\frac{1}{4}2^k\sqrt{\frac{k}{\log k}}$ yields the result.
\end{proof}
\begin{remark}
	Let $X_n = n^2$ with probability $1/n$ and $0$ otherwise. Note that $\prob{X_n > 0} = 1/n$, while $\Exp{X_n} = n$.
\end{remark}


\chapter{Alterations Method}
Up to this point, we made a random choice of object and use it. We now deal with the setting where a na\"{\i}ve random choice is not good enough - but we cat alter it a little bit so it would be good. The idea here is to bound the expectations of alterations needed to the random object.
\section{Dominating Sets}
\begin{yellowBox}
\begin{defn}
	Let $G$ be a graph. $A\subset V$ is \emph{Dominating} if any $v\in V$ has a neighbor in $A$.
\end{defn}	
\end{yellowBox}
\begin{thm}
	Let $G$ be of minimal degree $\delta$, then there exists a dominating set of size $n\cdot \frac{\ln(1+\delta}{1+\delta}$.
\end{thm}
\begin{proof}
	Let $B\subset V$ such that any $v\in B$ with probability $p$ (will be chosen later) independently. Let $C_B$ be the collection of vertices that all of their neighbors are not in $B$, that is $C_B = \cbk{x\notin B\mid \forall vx\in E\quad v\notin B}$. Clearly $A = B\cup C_B$ is dominating. Then
	\[
	\Exp{|A|} = \Exp{|B|} + \Exp{|C_B|} = np + n\prob{v\in C_B}\nim{\star}\le np ne^{-p(1+\delta)}
	\]
	With $\star$ since $\prob{v\in C_B} = (1-p)^{1+deg(x)}\le (1-p)^{1+\delta)} \le e^{-p(1+\delta)}$.
	 Find the optimal $p$ by differentiating w.r.t $p$,  and get $p = \frac{\ln(1+\delta)}{1+\delta}$, then $\Exp{|A|} \le n\rbk{\frac{\ln(1+\delta)+1}{1+\delta}}$
\end{proof}
\section{Ramsey Numbers - Revisited}
Recall that \ref{thm:Lower Bound Ramsey} gives us a lower bound on Ramsey numbers. We will use alterations to improve this lower bound.
\begin{thm}
	\label{thm: Ramsey Lower bound Improved}For any $n,k$, $R(k,k)\ge n-{n\choose k}\cdot 2^{1-{k\choose 2}}$
\end{thm}
\begin{proof}
	Consider $G\sim \Gg(n,\frac{1}{2})$. Note that $\Exp{\# \text{monochromatic sets of size }k}= {n\choose k}2^{1-{k\choose 2}}$ as we've seen, therefore there exists a graph with at most this amount of monochromatic sets of size $k$, denote it $G$. Let $G'$ be the graph obtained from $G$ by removing a single vertex of any monochromatic set of size $k$. Then $|V(G)|$ is at least $n-{n\choose k}\cdot 2^{1-{k\choose 2}}$, and clearly in $G'$ there is no monochromatic set of size $k$.
\end{proof}
\begin{cor}
$R(k,k) \ge n-\frac{e^n}{k}2^{1-{k\choose 2}}$ by the Stirling-esque estimation done in chapter $1$. The optimal $n$ is 	$\frac{2^{k/2}\cdot k}{e}$ which yields $R(k,k)\ge 2^{k/2}k\cdot\rbk{\frac{1+o(1)}{e}}$.
\end{cor}
\section{Girth and coloring}
\begin{yellowBox}
Let $G = (V,E)$ be a graph.
\begin{defn}
	[Girth] 
	The \emph{girth} of $G$ is the length of a minimal cycle in $G$. 
\end{defn}	
\begin{remark}
	In particular, if the girth is $\ge g$, then for any $v\in V$, its $g$-neighborhood looks like a tree.
\end{remark}
\begin{defn}
	[Chromatic Number] The \emph{chromatic number } of $G$, denoted $\chi(G)$ is the minimal $k$ such that there exists a proper coloring $c:V\to [k]$ of $G$.
\end{defn}
\begin{remark}
	It is difficult to know what $\chi(G)$ is - it is $\NPhard$
\end{remark}
\begin{defn}
	[Independence number] The \emph{Independence Number }of a graph $G$, denoted $\alpha(G)$, is the size of a largest independent set in $G$.
\end{defn}
\end{yellowBox}
\begin{claim}
	If $T$ is a tree, then $\chi(T) = 2$
\end{claim}
\begin{proof}
	It is bipartite - use BFS.
\end{proof}
\begin{thm}
	[Erd\H{o}s] \label{thm: High girth and chromatic}For any $k,g$ there exists a graph $G$ with $\chi(G) \ge k$ and girth $\ge g$.
\end{thm}
\begin{remark}
	This is surprising! Any neighborhood seems like $\chi$ should be small (as neighborhoods look like trees) - but it turns out it cannot be considered locally; $\chi$ is a \emph{global} property of $G$.
\end{remark}
For ease - we write $\alpha(G) = \alpha$, same for $\chi$.
\begin{lemma}
	$V(G)\le \alpha\cdot\chi$.
\end{lemma}
\begin{proof}
	If $c:V\to [\chi]$ is a proper coloring, any $c^{-1}(i)$ is independent.
\end{proof}
\begin{lemma}
	There exists a graph $G$ over $n$ vertices (for a large enough $n = n(k,g)$) with the following properties:
	\begin{enumerate}
  \item The number of cycles of length $\le g$ is smaller than $\frac{n}{2}$
  \item $\alpha(G) \le 3\log n\cdot n^{1-\frac{1}{2g}}$
\end{enumerate}
\end{lemma}
\begin{proof}
	Let $G \sim \Gg(n,p)$ with $p = n^{\frac{1}{2g}-1}$. Let $X$ be the number of cycles of length $\le g$. Then:
	
	\[
	\Exp{X}\nim{1}= \sum_{r=3}^g {n\choose r}\cdot \frac{(r-1)!}{2}\cdot p^r\nim{2}\le \sum_{r=3}^g (n\cdot p)^r \nim{3}\le g\cdot (n\cdot p)^g = g\sqrt{n}
	\]
	Justifications:
	\begin{enumerate}
  \item Choose which vertices are in a cycle of length $r$ (${n\choose r}$) and order them in a cycle ($(r-1)!/2$ options) and multiply by the probability of such cycle to exists.
  \item Bound ${n\choose r}\cdot \frac{(r-1)!}{2}$ from above naturally.
  \item Bound the sum with the largest element in the summation.
\end{enumerate}
Hence by Markov:
\[
\prob{X > n/2} \le \frac{g\sqrt{n}}{n/2}\nim{n\to \infty}\To 0
\]
Which implies the first property. For the second property, let $t = 3\log n\cdot n^{1-\frac{1}{2g}}$. Now:
\begin{displaymath}
  \prob{\alpha(G)\ge t}\le {n\choose t}(1-p)^{t\choose 2} \leq n^t\rbk{e^{-p}}^{t\choose 2} \le n^te^{-p\cdot{t\choose 2}} = e^{t(\log n - \frac{p\cdot t}{2} + 1)} = e^{t(-\frac{1}{2}\log n + 1)}\nim{n\to \infty}\To 0
\end{displaymath}


\end{proof}
\begin{proof}
	[Proof (of \ref{thm: High girth and chromatic})] Let $G'$ be a graph obtained from $G$ by removing a single vertex from any cycle of length smaller than $g$. Then $G'$'s girth is at least $g$. And $\alpha(G')\le \alpha(G)\le 3\log n\cdot n^{1-\frac{1}{2g}}$, and note that
	\[
	\chi(G')\ge \frac{|V(G')|}{\alpha(G')}\ge \frac{\frac{n}{2}}{3\log n\cdot n^{1-\frac{1}{2g}}}\ge \frac{n^\frac{1}{2g}}{6\cdot \log(n)}\nim{n\to \infty}\To \infty
	\]
\end{proof}

\section{Heilbronn triangle problem}
Let $P\subset [0,1]^2$, $|P| = n$, denote $T(P) = \underset{x,y,z\in P}{\min} Area(xyz)$, and let $T(n) = \underset{|P| = n}{\max} T(P)$. Heilbornn conjectured\footnote{falsely} that $T(n) = \Theta(\frac{1}{n^2})$.
\begin{thm}
	[KPS, no proof] $T(n) = \Omega\rbk{\frac{\log(n)}{n^2}}$
\end{thm}
\begin{remark}
	We still do not know some $f$ for which $T(n) = \Theta(f(n))$, the best upper bound is still not tight.
\end{remark}
\begin{thm}
	$T(n) \ge \frac{1}{70n^2}$
\end{thm}
\begin{proof}
	Let $\varepsilon=\frac{1}{70n^2}$. Generate $2n$ points $\sim U([0,1]^2)$ IID and remove a point from any triangle of area less than $\varepsilon$. Given a tringle $xyz$, Let $t$ be the distance $xy$. then $t$ has some density $f_{dist}(t)$. Then:
	
	\[
	\prob{area(xyz)\le \varepsilon} \l \int_0^{\sqrt{2}}\sqrt{2}\cdot 4\frac{\varepsilon}{t}f_{dist}(t)dt = (\star)
	\]
%	\todo{Add drawing from phone}
	
	Note that $f_{dist}(t) = \lim_{h\to 0}\frac{1}{h}\prob{t\le dist(x,y)) \le t+h}$ by the definition of density. Hence $f_{dist(x,y)}(t)\le \lim_{h\to 0}\frac{1}{h}\pi((t+h)^2 - t^2)) = 2\pi t$, then:
	\[
	(\star) \le \int_0^{\sqrt{2}} \sqrt{2}\frac{4\varepsilon}{t}2\pi t dt = 16\pi \varepsilon
	\]
	Which implies
	\[
	\Exp{\text{number of triangles with area smaller than $\varepsilon$}} \le {2n\choose 3} \frac{16\pi }{70n^2} < n
	\]
\end{proof}
\begin{remark}
	Erd\H{o}s has a non-combinatorial construction. Let $n$ be some prime, and consider the grid $[n-1]\times [n-1]$\footnote{Can rescale for the unit cube later...} and take $\cbk{(k,k^2\mod n)}_{k\in [n-1]}$. Note that the smallest triangle of $3$ points in $\ZZ^2$ is of area $1/2$, unless the three points are on the same diagonal. If they are on the diagonal $ax+b$, this means that there exists three values of $k$ such that $(ak+b) = k^2 \mod n$, but this is a quadratic polynomial in $\FF_n[x]$, therefore it cannot have more than $2$ solutions. Hence by scaling, $T(n) \ge \frac{1}{2(n-1)^2}$
\end{remark}
\chapter{Second Moment Method}
Up until now we discussed \emph{first moment methods}. More formally, if $X = X_n\ge 0$ is an integer valued random variable, then the first moment method tells us that if $\Exp{X_n}\nim{n\to \infty}\To 0$, then $\prob{X_n>0}\nim{n\to\infty}\To 0$. 
\begin{example}
	[First Moment Method] When $G\sim \Gg(n,p)$ is triangle-free? Denote $X$ the number of traiangels in $G$. Then
	\[
	\Exp{X} = {n\choose 3}p^3\le (np^3)
	\]
	Then taking $p = o\rbk{\frac{1}{n}}$ results in $\prob{X > 0}\nim{n\to 0}\To 0$. Is this bound \emph{tight}? We saw that the expectation does not always give us a good bound - we need a way to reason about when is $X$ concentrated about its expectation - that is the variance.
\end{example}
\begin{yellowBox}
\begin{defn}
	[Variance] The variance of $X$ is $Var{X} = \Exp{\rbk{X-\Exp{x}}^2}$
\end{defn}	
\begin{defn}
	[Covariance] The covariance of $X,Y$ is $$cov{X,Y} = \Exp{(X-\Exp{X})\cdot(Y-\Exp{Y})} = \Exp{XY} - \Exp{X}\cdot\Exp{Y}$$
\end{defn}
\begin{thm}
	[Chebyshev]\label{thm:Chebyshev ineq} $\prob{|X-\Exp{X}|\ge t}\le \frac{Var{X}}{t^2}$
\end{thm}
\begin{cor}
	$\prob{X = 0}\le \frac{Var(X)}{\Exp{X}^2}$
\end{cor}
\begin{cor}
If $Var{X} = o\rbk{\Exp{X}^2	}$ then $\prob{X=0}\To 0$
\end{cor}

\end{yellowBox}
This results in the \emph{Second moment method}: If $Var{X} = o\rbk{\Exp{X}^2}$ then $\prob{X\ge 0}\nim{n\to\infty}\To 0$. An equivalent condition is $\Exp{X^2} = \Exp{X}^2(1+o(1))$\footnote{Since $Var{X} = \Exp{X^2} - \Exp{
X}^2$}
An important case is when $X = \sum_{i=1}^mX_i$, in that case \[
Var{X} = \sum_{i=1}^m Var{X_i} + \sum_{i=1}^m\sum_{j\neq i}cov{X_i,X_j}
\]
If we denote $i\sim j$ when $X_i,X_j$ are dependent, then
\[
Var{X} = \sum_{i=1}^m Var{X_i} + \sum_{i=1}^m\sum_{j\sim i}cov{X_i,X_j}
\]

\paragraph{Assumptions:}
\begin{enumerate}
	\item $X_i = \one_{A_i}$, then $Var{X_i} = \prob{A_i}\cdot(1-\prob{A_i})\le \prob{A_i}$ and $Cov{X_i,X_j} = \prob{A_i\cap A_j} - \prob{A_i}\cdot \prob{A_j}\le \prob{A_i\cap A_j} = \prob{A_i}\cdot \prob{A_j\mid A_i}$. Under this assumption, we get 
	\[
	Var{X}\le \Exp{X} + \sum_i\prob{A_i}\cdot\sum_{i\sim j} \prob{A_j\mid A_i}
	\] 
	\item A symmetry assumption is $\sum_{i\sim j} \prob{A_j\mid A_i}$ is independent of $i$. This is usually true in many cases. We denote $\sum_{i\sim j} \prob{A_j\mid A_i} = \Delta^*$\footnote{Usually $ \sum_i\prob{A_i}\cdot\sum_{i\sim j} \prob{A_j\mid A_i} \defeq \Delta$}. With this notation, $Var{X} \le \Exp\cdot(1+\Delta^*)$.
	\begin{cor}
	If $\Exp{X}	\To \infty$, $\Delta^* = o\rbk{\Exp{X}}$, then $\prob{X=0}\To 0$.
	\end{cor}
\end{enumerate}
\section{$H$-free graphs}
The general question we deal with is
\begin{question}
	Given a small graph $H$, what is the threshold function of $G\sim \Gg(n,p)$ containing $H$?
\end{question}
More formally:
\begin{yellowBox}
\begin{defn}
	[Threshold Function] Let $G\sim \Gg(n,p)$ and $H$ some fixed small graph. We say $f(n)$ is a \emph{threshold function} for finding $H$ in $G$ if
	\begin{align*}
		& p\ll f(n) \Rightarrow \prob{G\text{ contains a copy of }H}\nim{n\to\infty}\To 0 \\
		&{\text{\centering and}} \\
		&p\gg f(n) \Rightarrow \prob{G\text{ contains a copy of }H}\nim{n\to\infty}\To 1 \\
	\end{align*}
\end{defn}	
\end{yellowBox}
We now explore some threshold functions
\subsection{Triangles in $\Gg(n,p)$}
We've shown that if $G\sim \Gg(n,p)$ and $p \ll \frac{1}{n}$ then $\prob{G\text{ contains a triangle}}\to 0$.
\begin{claim}
	If $p \gg \frac{1}{n}$ then $\prob{G\text{ contains a triangle}}\to 1$
\end{claim}
\begin{remark}
In a case like this, we say that $\frac{1}{n}$ is the \emph{threshold function} for triangle existence in $\Gg(n,p)$.
\end{remark}
\begin{proof}
	Denote by $X$ the number of triangles in $G$, then:
	\[
	\
	\Exp{X} = {n\choose 3}p^3 = (1_o(1))\frac{n^3p^3}{6}\nim{p\gg \frac{1}{n}}\To\infty
	\]
	Denote by $T$ a triangle in $G$.
	\[
	\Delta^* = \sum_{T'\sim T}\prob{A_{T'}\mid A_T} \nim{\star}= 3\cdot(n-3)p^2\le 3np^2
	\]
	With $\star$ since any $T'$ dependent on $T$ is taken by choosing $2$ vertices in $T$ and a vertex not in $T$. Now we have
	\[
	\frac{\Delta^*}{\Exp{X}} \le \frac{18}{n^2p}\To 0
	\]
	And we are done.
\end{proof}
\begin{remark}
In fact we've shown that $\frac{X}{\Exp{X}}\nim{\text{in probability}}\To 1$. This is some kind of law of large numbers. This can be seen by \autoref{thm:Chebyshev ineq}:
\[
\prob{|X-\Exp{X}> \varepsilon \Exp{X}}\le \frac{Var{X}}{\varepsilon^2\Exp{X}^2}\To 0
\]
\end{remark}

\subsection{$K_4$ in $\Gg(n,p)$}
Denote $X$ the number of $K_4$ copies in $G$. Then
\[
\Exp{X} = {n\choose 4}p^6 = \frac{1+o(1)}{24}n^4p^6
\]
Then if $p \ll n^{-2/3}$, $\prob{X = 0}\To 0$, and otherwise $\Exp{X}\to \infty$. Now denote by $S$ a fixed copy of $K_4$ in $G$. Then:
\begin{flalign*}
	\Delta^* = \sum_{S'\sim S}\prob{A_{S'}\mid A_S}\le \overbrace{6n^2p^5}^\text{Share $2$ vertices} + \overbrace{4np^3}^\text{Share $3$ vertices}
\end{flalign*}
Then
\[
\frac{\Delta^*}{\Exp{X}} \le O\rbk{\frac{1}{n^2p} + \frac{1}{n^3p^3}}\nim{p\gg n^{-2/3}}\To 0
\]
\subsection{$K_4*e$ in $\Gg(n,p)$}
In this case
\[
\Exp{X} = 5{n\choose 5}p^7 = \frac{1+o(1)}{24} n^5p^7
\]
Then $p\ll n^{-5/7}$ implies $\prob{X > 0}\To 0$. Is it true that $p\gg n^{-5/7}$ implies $\prob{X = 0}\To 1$? Note that $n^{-5/7} \ll n^{-2/3}$, and since existence of $K_4*e$ implies existence of $K_4$, had $n^{-5/7}$ been the threshold, it would contradict our previous proof.
\begin{yellowBox}
\begin{defn}
	[Maximal Subgraph Density] Given $H$, we define its maximal density by
	\[
	m(H) \defeq \underset{\emptyset\neq A\subset V(H)}{\max} \frac{e(A)}{|A|}
	\]
\end{defn}	
\begin{remark}
	First moment argument shows that $p\ll n^{-\frac{1}{m(H)}}$.
\end{remark}
\end{yellowBox}
\begin{thm}
	[Threshold characterization]\label{thm: Threshold characterization} The threshold function of $H$ is $n^{-\frac{1}{m(H)}}$
\end{thm}
\section{Cliques in $\Gg(n,1/2)$}
\begin{thm}
	Let $G\sim \Gg(n,1/2)$. Denote $X =\text{size of maximal clique in $G$}$.  There exists $k = k(n)$ and $k = \Theta(2\log_2(n))$ such that 
	\[
	\prob{X\in \cbk{k,k+1}} \nim{n\to \infty}\To 1
	\]
\end{thm}
\begin{proof}
	[Proof (Sketch).] Define 
	$f(k) = \Exp{\#\text{of $k$-cliques in $G$}} = {n\choose k}2^{-{k\choose 2}}$. We claim that if $f(k)\to \infty$ then there exists a clique of size $k$.
	\[
	\Delta^* =\sum_{i=2}^{k-1}{k\choose i}\cdot{n-k\choose k-i}\rbk{\frac{1}{2}}^{{k\choose 2}-{i\choose 2}}
	\]
	Now it can be shown that $k\sim 2\log_2(n$, if $f(k)\to \infty$ then $\frac{\Delta^*}{f(k)}\to 0$ by case analysis. We note that 
	\[
	\frac{f(k+1)}{f(k)} = \frac{{n\choose k+1}2^{-\frac{k(k+1)}{2}}}{{n\choose k}2^{-\frac{k(k-1)}{2}}} = \frac{n-k}{(k+1)}2^{-k}\approx 2^{-2\log_2(n)} < \frac{1}{n}
	\]
	We now consider a $k_0$ such that $f(k_0)\ge 1$ and $f(k_0+1) < 1$. If $f(k_0(n))$ tends to $\infty$ with $n$ and $f(k_0) = o(n)$, then $f((k_0+1)(n))\nim{n\to \infty}\To 0$. In this case, there exists a maximal $k_0$-clique, and no $(k_0+1)$-clique, so the maximal clique of $k_0$. A similar argument may show the result, but it's quite annoying. The book has the complete proof
\end{proof}

\section{Distinct Sums Problems}
The \emph{Distinct Sums Problems} is a problem suggested by Erd\H{o}s\footnote{And it is still open with a $\$ 300 $ prize awaiting to the solver!}.

\begin{problem}
What is the maximal size of $S\subset [n]$ with distinct partial sums?	
\end{problem}
\begin{sol}[lower bound] Take $S = \{1,2,4,8,\ldots\}$, then $|S| = \log_2(n)$. 
\end{sol}
\begin{question}
	Is $|S|\le \log_2(n) + o(1)$?
\end{question}
\begin{sol}
[Upper bound] $2^{|S|}\le n\cdot |S|$
\end{sol}
\begin{cor}
$|S| \le \log(n) + \log\log(n) + o(1)$
\end{cor}
\begin{claim}\footnote{This is worth $\$150 $!}
$|S| \le \log(n) + 0.5\log\log(n) + o(1)$	
\end{claim}

\begin{proof}
	Let $S = \{s_1\ldots s_m\}$ and consider a random partial sum $X = \sum_{i=1}^m b_is_i$ with $b_i\sim U(\{0,1\}$.
	\[
	\mu \defeq\Exp{X} =\sum_{i=1}^m\frac{s_i}{2}\quad\text{and}\quad  Var{X}\sum_{i=1}^m\frac{s_i^2}{4}
	\]
	Now try to bound the variation:
	\begin{flalign*}
		Var{X} = \frac{1}{4}\sum_{i=1}^ms_i^2\nim{s_i\le n}\le \frac{mn^2}{4}
	\end{flalign*}
	Then
	\begin{flalign*}
		& \prob{|x-\mu|\le \lambda}\nim{Chebyshev}\ge 1-\frac{mn^2}{4\lambda^2}\\
		& \prob{|x-\mu|\le \lambda} \nim{\text{distinct sums}}\le (2\lambda + 2)2^{-m}
	\end{flalign*}
	So
	\begin{flalign*}
		 & (2\lambda + 2)2^{-m} \ge 1-\frac{mn^2}{4\lambda^2}\qquad \nim{\text{Take }\lambda = \sqrt{3}\sqrt{m}n}\iff\\
		 &c\sqrt{m}n2^{-m}\ge\frac{4}{c'mn^2} \geq \frac{11}{12}
	\end{flalign*}
	Therefore $2^{m}\le \tilde{C}\sqrt{m	}n$ And some computations result in the bound.
\end{proof}

\section{Hardy- Ramanujan Thoerem}
The question we deal with is "how many prime numbers divide a random number $\sim U([n])$? 
\begin{thm}
	[Hardy- Ramanujan]\label{thm:Hardy- Ramanujan}
	For $x\in \NN$ let $\nu(x)$ be the number of prime divisors of $x$ (without multiplicity). If $x\in U([n])$, then for a large enough $n$,
	\[
	\forall\varepsilon > 0 \quad \exists A>0 \quad \prob{\abs{\nu(x) - \log\log n} > A\sqrt{\log\log n}} < \varepsilon
	\]
\end{thm}
\begin{thm}
	[Erd\H{o}s-Kac]\label{thm: Erdos-Kac}
	With the same notations,
	\[
	\frac{\nu(x) - \log\log n}{\sqrt{\log\log n}}\nim{\Dd}\To \Nn(0,1)
	\]
\end{thm}
\begin{thm}
	[Merten]\label{thm:Merten} $\sum_{p\le m} \frac{1}{p} = \log\log M = O(1)$
\end{thm}
\begin{proof}
	[Proof(of\autoref{thm:Hardy- Ramanujan}).] Denote by $X(x)$ the amount of prime divisors of $x$ not greater then $n^{1/10}$. Clearly $X\le \nu$. Since there cannot be more than $10$ divisors larger than $n^{1/10}$\footnote{Let $m$ be the number of divisors larger than $n^{1/10}$, then $x$ is at least $n^{m/10}\le x\le n$.}, we have $\nu - 10 \le X$. Denote $\chi_p \one_{p\mid x}$. Then:
	\begin{flalign*}
		&X = \sum_ {p\le n^{1/10} } \chi_p\quad \text{ and }\quad  \Exp{\chi_p} = \frac{\Floor{\frac{n}{p}}}{n} = \frac{1}{p} + O(1/n)\Rightarrow \\
		&\Exp{X} = \sum_{p\le n^{1/10}} \frac{1}{p} + O(1) \nim{\text{\autoref{thm:Merten}}}=\log\log n + O(1) 
	\end{flalign*}
	Now we calculate the $\Delta^*$ part. For any $p,q$ primes, we have $\prob{\chi_p\cdot\chi_q} = \frac{\Floor{\frac{n}{pq}}}{n}$ (by the Chinese Reminder Theorem), therefore:
	\begin{flalign*}
		cov(\chi_p,\chi_q) = \Exp{\chi_p\chi_q} - \Exp{\chi_p}\Exp{\chi_p} =\frac{\Floor{\frac{n}{pq}}}{n} - \frac{\Floor{\frac{n}{p}}}{n}\frac{\Floor{\frac{n}{q}}}{n} = \frac{1}{pq} - \rbk{\frac{1}{p}-\frac{1}{n}}\rbk{\frac{1}{q}-\frac{1}{n}}\le \frac{1}{n}\rbk{\frac{1}{p} + \frac{1}{q}}
	\end{flalign*}
	So
\begin{flalign*}
	& Var{X} = \sum_p Var{\chi_p} + \sum_{p\neq q} cov{\chi_p,\chi_q} \le \log\log n + O(1) + \sum_{p\neq q}\frac{1}{n}\rbk{\frac{1}{p} + \frac{1}{q}} \le \\
	& \log\log n + O(1) +(n^{1/10})^2\frac{1}{n} = \log\log n + o(1)
\end{flalign*}
So the result follows from Chebyshev.
\end{proof}
\section{The R\"{o}de Nibble}
The question deals with the existence of designs.
\begin{yellowBox}
\begin{defn}
	[$(n,k,r)$ - design]: An $(n,k,r)$ - design is a $k$-graph over $n$ vertices such that any $r$-set of vertices is contained exactly in one edge. 
\end{defn}	
\end{yellowBox}
\begin{example}
	$k=2, r=1$ means a perfect matching. 
\end{example}
\begin{question}
	Is is true that for any $r < k \ll n$ there exists a corresponding design?
\end{question}
Of course not! Take an odd $n$, one cannot find a perfect matching in a graph over an odd number of vertices. We get division requirements: Denote by $e$ the number of edges in a design. Then
\[
e\cdot{k\choose r} = {n\choose r} \Rightarrow {k\choose r}\mid {n\choose r}
\]

\begin{yellowBox}
\begin{defn}
	The Complementary design with respect to $A\subset [n]$ with $|A| < r$ is with the edges
	\[
	E_A = \cbk{e\setminus A\mid A\subset e \in E}
	\]
\end{defn}
\end{yellowBox}
\subsection{Approximations}
\begin{yellowBox}
\begin{defn}
	[Covering] a covering is a relaxation of designs, when we demand that any $r$-tuple is contained in \emph{at least} one edge
\end{defn}	
\begin{defn}
	[Packing] a covering is a relaxation of designs, when we demand that any $r$-tuple is contained in \emph{at most} one edge
\end{defn}	
\end{yellowBox}
\begin{remark}
	In theres cases, clearly $|E| \ge \frac{{n\choose r}}{{k\choose r}}$ and $|E| \le \frac{{n\choose r}}{{k\choose r}}$ respectively.
\end{remark}
The \emph{Erd\H{o}s Hananni conjecture} is that for any $k,r$, when $n\to\infty$ there exists a covering of size\footnote{In the sense of $|E|$} $(1+o(1))\frac{{n\choose r}}{{k\choose r}}$. This is equivalent of having a packing of size $(1-o(1))\frac{{n\choose r}}{{k\choose r}}$. This conjecture was proved by using the \emph{R\"ode Nibble}.
\begin{proof}
	[Proof (for the case $r=2,k=3$)]\footnote{Steiner Triplet systems} We look for a collection of $(1+o(1))\frac{{n\choose 2}}{3}$ triplets that cover all edges. Had we tried to choose any triangle independently with probability $1/n$, we would have failed miserably: \[
	\prob{\text{A specific edge is not covered}} = \rbk{1-1/n}^{n-2}\approx \frac{1}{e}
	\]
	Which means that this method "misses" a constant amount of edges!\\
	We try to choose any trianegl with probability $\frac{\varepsilon}{n}$, which results in approximately $\frac{\varepsilon n^2}{6}$ triangles. Then:
	\[
		\prob{\text{A specific edge is not covered}} = \rbk{1-1/n}^{n-2}\approx \frac{1}{e^\varepsilon}
	\]
	So
	\[
	\prob{\text{a specific edge is covered}} \approx 1-e^{-\varepsilon} \approx\varepsilon - \frac{\varepsilon^2}{2}\ldots
	\]
\end{proof}
\begin{defn}
	[Typical Graph] A graph with $m$ edges is called ($D,\delta,k)$-\emph{Typical} if:
	\begin{enumerate}
		\item Aside from $\delta\cdot m$ edges, all edge is contained in $D(1\pm\delta)$ triangles. 
		\item Any edge is contained in at most $kD$ triangles.
	\end{enumerate}
\end{defn}
\begin{lemma}
	For any $\varepsilon > 0$, large enough $D$, $k$ and $\delta>0$, there exsists $\gamma > 0$ such that in any $(D,\delta,k)$ typical graph there is a collection of $\frac{\varepsilon}{3}(m\pm \gamma)$ triangles, denoted $T$ such that $G\setminus T$ is a graph with $m\cdot e^{-\varepsilon}(1\pm \gamma)$ edges, and is $(De^{-2\varepsilon},\gamma,ke^{2\varepsilon})$-typical
\end{lemma}
\begin{proof}
	We sample each triangle i.i.d with probability $\frac{\varepsilon}{D}$. The number of triangles in the graph is at least $\frac{(1-\delta)mD(1-\delta)}{3} = \frac{mD}{3}(1-\delta_1)$, and at most $\frac{\delta mkD + (1-\delta )mD(1+\delta)}{3} = \frac{mD}{3}(1+\delta_1)$. Let $T$ be the number of triangles. Then $T\sim \Bin{\frac{mD}{3}(1\pm \delta_1),\frac{\varepsilon}{D}}$ and the first item in the definition is gained by first moment argument.\\
	Let $X_e$ be the indicator of the event "e is not covered". Then if $d_e = D(1\pm \delta)$\footnote{The number of triangles containing $e$}, we get
	\[
	\Exp{X_e} = \rbk{1-\frac{\varepsilon}{D}}^{D(1\pm\delta)} = e^{-\varepsilon}(1+\delta_1)
	\] Let $X = \sum_{e} X_e$, then $\Exp{X} = me^{-\varepsilon}(1-\delta_1)$ and 
	\begin{align*}
		&cov{X_e,X_{e'}} = \prob{\text{both not covered}} - \prob{e\text{ is not covered}}\prob{e'\text{ is not covered}} \\
		& \rbk{1-\frac{\varepsilon}{D}}^{d_e+d_{e'}-1} -  \rbk{1-\frac{\varepsilon}{D}}^{d_e} \rbk{1-\frac{\varepsilon}{D}}^{d_{e'}} \le \frac{\varepsilon}{D}
	\end{align*}
	Then
	\begin{align*}
		&Var{X} \le me^{-\varepsilon}(1\pm \delta_1) + mD(1+\delta)2\frac{\varepsilon}{D} = O(m)
	\end{align*}
	Then by Chebyshev, $\prob{\text{The number of edges}\notin me^{-\varepsilon}(1\pm \delta_2) < 0.01} \to 0$. It is left to show that $G\setminus T$ is typical. 
	\begin{claim}
		Other than $\delta_1m$ edges, all edges are both good and contained in $(1\pm \delta_1)D$ triangles whose edges are good.
	\end{claim}
	\begin{proof}
	\begin{flalign*}
		& \Exp{d_e(G\setminus T)} = (1\pm \delta_1)De^{-2\varepsilon}(1\pm \delta_1)^2\\
		& Var{d_e(G\setminus T)} \le \Exp{d_e(G\setminus T)} + D^2\frac{\varepsilon}{D} = O(D)
	\end{flalign*}
	Wo once again, Chebyshev we are done.
	\end{proof}

\end{proof}

\begin{proof}
	[Proof (R\"ode's Nibble - general case).] Denote $p = e^{-\varepsilon}$Let $G_0 = K_n$. Let $G_{i+1}$ be obtained from $G_i$ by removing each triangle with probability $\frac{\varepsilon}{p^{2i}D}$ .Then $|E(G_i)| \approx p^i{n\choose 2}$: With this step we've chosen $$\approx \frac{\varepsilon}{p^{2i}D}\cdot \overbrace{p^i{n\choose 2}}^{|E(G_i)|}\cdot  \overbrace{\frac{p^{2i}n}{3}}^{\text{\# $\triangle$ in typical edge}} = \varepsilon p^i\frac{n^2}{6}$$
	Hence the number of triangles in the cover is
	\[
	p^t{n\choose 2} + \sum_{i=0}^{t-1}\varepsilon p^i\frac{n^2}{6} \le \frac{n^2}{6}\rbk{3e^{-\varepsilon t} + \varepsilon\frac{1}{1-e^{-\varepsilon}}} = \star
	\]
	So when $\varepsilon\to 0$, we can choose a large enough $t$, we may have $\star \le (1+\delta)\frac{n^2}{6}$. The thing is - we've hidden all the error terms, but this can be dealt with. Super annoyingly.
\end{proof}

\chapter{Lov\'asz's Local Lemma}
Up to this point we used probability to find an object of interest with high probability.  The \emph{Local Lemma} is a tool to prove an object's existence even if the probability of finding them is small - even exponentially. In fact, this is an \emph{algorithmic} approach.
\begin{yellowBox}
\begin{thm}
	[The local Lemma, Symmetric]\label{Local Lemma} Let $(A_i)_{i\in [n]}$ be events such that:
	\begin{enumerate}
  \item  $A_i$ is independent in all $A_j$, except for at most $d$ of them\footnote{Not in pairs! $A_i$ is dependent on the event $\cup_{j\in K} A_j$ for some $K\in {[n]\setminus i\choose d}$}
	\item $\prob{A_i} \le p$
	\item $e\cdot p\cdot (d+1)\le 1$
\end{enumerate}
Then $\prob{\cap_{i\in [n]}\overline{A_i}} > 0$
\end{thm}	
\end{yellowBox}
\section{Results from the lemma}
\begin{thm}
	[Improvement on \autoref{thm:Ramsey}] If $e\cdot 2^{1-{k\choose 2}}\rbk{{k\choose 2}{n\choose k-2} + 1}<1$, then $R(k,k) > n$
\end{thm}
\begin{proof}
	Let $G\sim \Gg(n,1/2)$. For any $S\in {[n]\choose k}$, denote $A_S$ the even that $S$ is a clique or an anti-clique. We know that $\prob{A_S} = 2^{1-{k\choose 2}}$. Note that $A_S$ is independent in all  $A_M$ other than at most $ {k\choose 2}{n\choose k-2}$. Then by the \autoref{Local Lemma} - there exists a graph in which no $A_S$ occures. 
\end{proof}
\begin{exercise}
\label{Ex local lemma}
Consider a $k$-SAT in which any variable appears in at most $r$ clauses ($k>3r$). Show a polynomial algorithm to decide satisfyability.
\end{exercise}
\begin{thm}
	Any $k$-graph (a $k$-uniform hypergraph) in which any edge intersects at most $\frac{2^{k-1}}{e} - 1$ other edges is $2$-colorable.
\end{thm}
\begin{proof}
	Consider a random $2$ coloring $c: X\to \cbk{0,1}$. Assume $A_i$ is the event "the $i$'th edge is monochromatic", then $\prob{A_i} = 2^{1-k}$, and $d = \frac{2^{k-1}}{e}-1$, and the result follows from LLL\footnote{Lov\'az's Local Lemma}.
\end{proof}
\subsection{Colorings of $\RR$}
Consider a coloring $c:\RR\to [k]$. We say $T$ is \emph{Colorful} if $c[T] = [k]$. The question Lov\'{a}s and ??? asked is given a finite $S$, can we color $\RR$ such that $S$ \emph{and all of its translations} are colorful.
\begin{thm}
	For any $k$ and for any $S$ of cardinality $m$ such that $$e\cdot k\rbk{1-\frac{1}{k}}^m(m(m-1)+1)\le 1$$  there exists a coloring $c:\RR\to [k]$ such that all of $S$'s translations are colorful.\footnote{Doing the calculations, we get $m \approx(3+o(1))k\log k$}
\end{thm}
\begin{proof}
	Denote $c_x = \cbk{c\mid x + S \text{ is colorful}}$. We want to show that $\bigcap_{x\in \RR} c_x \neq \emptyset$. By compactness arguments, it is sufficient to show that for any finite $X$, $\bigcap_{x\in X}c_x \neq \emptyset$. Consider a random coloring $c:\RR\to [k]$. For any $x\in X$, the event $A_x = $ "$x+S$ is not colorful", hence 
	\[
	\prob{A_x}\le k\rbk{1-\frac{1}{k}}^m
	\]
	And we note that $A_x,A_y$ are independent unless $(x+S)\cap (y+S) \neq \emptyset$, hence there are at most $m(m-1)$ such $y$'s for which it happens. From LLL we are done.
\end{proof}
\subsection{Coverings of $\RR^3$}
\begin{yellowBox}
	\begin{defn}
		[$k$-covering] A $k$ covering of a metric space $X$ is a covering in which any element is in at least $k$ covering sets.
	\end{defn}
	\begin{defn}
		[Reducible] We say a covering $\Uu$ is \emph{reducible} if it can be partitioned into two coverings $\Uu_1,\Uu_2$ that are disjoint in their open sets.
	\end{defn}
\end{yellowBox}
\begin{question}
	Is there a $k$-covering in which any point is covered exactly $k$ times? $O(k$ times?
\end{question}
\begin{thm}
	[Nani-Levicko, Pach. No Proof] For any $k$ there exists an irreducible $k$-covering of $\RR^3$ by unit balls.
\end{thm}
\begin{thm}
	Any $k$ covering in which any point is covered at most $t\defeq c\cdot 2^{\frac{k}{3}}$ times is reducible.
\end{thm}
\begin{proof}
	Given a covering $\Uu = \cbk{B_i}_{i\in I}$, define a hypergraph $H$ with vertex set $\Uu$ and edges indexed by $\RR^3$: for any $x\in \RR^3$, $e_x = \cbk{B_i\mid x\in B_i}$ and delete multiple edges. That is, edges correspond to "cells" in $\Uu$. for any $x$, $k\le |e_x|\le t$. We need to show that $H$ is $2$ colorable, which will correspond to two subcoverings. It suffices to show that any finite subgraph of $H$ is $2$-colorable (by compactness). Consider a random $2$-coloring $c$, denote by $A_x$ the event $e_x$ is monochromatic, then $\prob{A_x}\le 2^{1-k}$. If $A_x,A_y$ are dependent, then $d(x,y) \le 4$: If $d(x,y) > 4$, any two balls containing $x,y$ do not intersect. We now claim that any edge $e_x$ intersects at most $c\cdot t^3$ other edges. By the previous claim - any ball intersecting some ball containing $x$ is contained in $ B_4(x)$, and any point is covered at most $t$ times - thus the sum of volumes of balls intersecting some ball with $x$ is $N \le 4^3\cdot B_1 \approx 4^3$. The number of sells is hence at most $N^3\nim{\text{exercise}}\le 4^9t^3$, thus by LLL $H$ is $2$-colorable if $e\cdot 2^{1-k}(4^9t^3 + 1)\le 1$: choosing $c$ appropriately guarantees this.
\end{proof}
\begin{remark}
	If we start with a $2k$ cover and we want to partition into two $k$-coverings, this happens w.h.p polinomilally. From $3k$ to two $k$-coverings, we need to bound $$\prob{\text{Less than $k$ blue or less than $k$ red}} \ge 2\cdot \lambda^{-k}$$, and we get exponential h.p.
\end{remark}
\section{Proof of the Local Lemma}
\begin{yellowBox}
\begin{defn}
	[Dependencies Graph] Let $\Aa = \cbk{A_1\ldots A_n}$ be events in some probability space. The \emph{Dependencies Graph} of $\Aa$ is the DiGraph with $V = \Aa$ and $A_i$ is independent of all $A_j$ such that $A_iA_j\notin E$. We identify $A_i$ with $i$.
\end{defn}
\end{yellowBox}
\begin{thm}
	[The True Local Lemma]\label{lemma: Local Lemma restate} Let $\Aa$ be some events with dependencies graph $\Dd$. If there exists $0<x_i < 1$ with 
	\[
	\prob{A_i}\le x_i\cdot \prob{i\to_{\Dd} j}(1-x_j)
	\]
	then
	\[
	\prob{\bigcap_{i\in [n]}\overline{A_i}} \ge \prod_{i\in [n]}(1-x_i)
	\]
	In particular, if $\Aa$ are pairwise independent, then $\prob{\bigcap_i\overline{A_i}} = \prod_i(1-x_i)$. In the symmetric case, taking $x_i = \frac{1}{d+1}$, by assumption $e\cdot p\cdot(d+1)\le 1$ we have
	$\prob{A_i}\le p\le \frac{1}{e\cdot(d+1)}\le x_i \rbk{1-\frac{1}{d+1}}^d$.
\end{thm}	
\begin{remark}
	This implies \autoref{Local Lemma}
\end{remark}
\begin{proof}
	For any $i$, for any $i\notin S\subset [n]$, $\prob{A_i\mid \cap_{j\in S}\overline{A_j}}\le x_i$. This implies the lemma, since 
	\[
	\prob{\bigcap \overline{A_i}} = \prod_{i=1}^n\prob{\overline{A_i}\mid \bigcap_{j=1}^{i-1}\overline{A_i}}\ge \prod_i(1-x_i)
	\]
	So we prove the claim by induction on $|S|$:\\
	$|S| = 0$ we get from the assumption.\\
	Define $S_1= \cbk{j\in S \mid (i,j)\in \Dd}$ and $S_2 = S\setminus S_1$, and let $B = \bigcap_{j\in S_1}\overline{A_j}$ and $C = \bigcap_{j\in S_2}\overline{A_j}$. Then:
	\[
	\prob{A_i\mid B\cap C} = \frac{\prob{A_i\cap B\mid C}}{\prob{B\mid C}}\le \frac{\prob{A_i \mid C}}{\prob{B\mid C}} = \frac{\prob{A_i}}{\prob{B\mid C}}\le \frac{x_i\prod_{i\to j}(1-x_j)}{\prob{B\mid C}}
	\]
	So we need to show $\prob{B\mid C}\ge \prod_{j\in S_1}(1-x_j)$. Denote $S = \cbk{j_k}_{k\in [t]}$, then:
	\begin{align*}
		& \prob{B\mid C} = \prod_{k\in [t]}\prob{\overline{A_{j_k}}\mid \prod_{k'=1}^{k-1}\overline {A_{j_{k'}}}\cap C}\nim{\text{induction }}\ge \prod_{k=1}^t(1-x_{j_k}) = \prod_{j\in S_1}(1-x_j)
	\end{align*}
	
	\end{proof}
	\subsection{The Algorithmic Version of the Lemma}
	Assume $A_1\ldots A_n$ are events in some product space $\Sigma^N$ That satisfy the conditions of \autoref{lemma: Local Lemma restate}. Is it possible to efficiently find $(\sigma_1,\ldots ,\sigma_N)\in \Sigma^N$ such that no $A_i$ holds? This is a generalization of \autoref{Ex local lemma}.
	\paragraph{The Moser-Tard\"os Algorithm for $SAT$:} Consider a  random $\sigma$. If some $A_i$ holds - resample all of the $\sigma_i$ on which he is dependent.
	\begin{claim}
		The expected number of times $A_i$ is resampled is at most $\frac{x_i}{1-x_i}$ - so the runtime of the algorithm is linear.
	\end{claim}
	\begin{proof}
		No forml Proof. Consider the "log"\footnote{Computer-wise} of the events we've taken care of: $A,B,A,C,B,D,C,A'\dots$, and we ask "why is $A'$ resampled?" - we build a tree rooted at $A'$ - one of its predecessors in the log "broke it", say $C$ (so $C$ is a child of $A'$ in the tree), and maybe $D$ as well. Continue in this manner. We ask the probability for such a tree to occurre - and bound this. In fact, $\sum_{T\text{ Tree}}\prob{T} \le \frac{x_I}{1-x_i}$
	\end{proof}
	\chapter{Concentration of Measure and Martingales}
	\section{Chernoff Bound}
	\begin{thm}
		[Chernoff Bound, Hoeffding]\label{thm: Chernoff Bound} Let $X_1\ldots X_n\underset{i.i.d}{\sim}\cbk{\pm1}$, then
		\[
		\prob{\sum_{i=1}^n X_i > t}\le e^{-\frac{t^2}{2n}}
		\]
		In particular - $t = \alpha\sqrt{n}$ then the bound is $ e^{-\frac{\alpha^2}{2}}$, like the Gaussian Tail. If $t = \alpha\cdot n$ then the bound is $e^{-\frac{\alpha^2n}{2}}\nim{n\to\infty}\To 0$ exponentially.
	\end{thm}	
	\begin{remark}
		This can be generalized: Let $X_1\ldots X_n$ be Bernouli independent RV, with $\mu = \Exp{\sum X_i}$, then
		\[
		\prob{\sum X_i > (1+\varepsilon)\mu} \le e^{-C_\varepsilon \mu^2}
		\]
	\end{remark}
	\begin{proof}
		\begin{flalign*}
			&\prob{\sum_{i=1}^n X_i > t} \nim{\lambda > 0}= \prob{e^{\lambda\sum_{i=1}^n X_i} > e^{\lambda t}}\nim{\text{Markov}}< \frac{\Exp{e^{\lambda\sum_{i=1}^n X_i}}}{e^{\lambda t}} = \\
			&\frac{\Exp{\prod_{i=1}^ne^{\lambda X_i}}}{e^{\lambda t}}\nim{\text{i.i.d}}= \frac{\Exp{e^{\lambda X_1}}^n}{e^{\lambda t}}
		\end{flalign*}
		Note that $\Exp{e^{\lambda X_1}}^n$ is the \emph{Moment Generating Function}. Since $\Exp{e^{\lambda X_1}} = e^{\frac{\lambda^2}{2}}$, plugging it in results in
		\[
		e^{\frac{\lambda^2}{2}n - \lambda t} \nim{\lambda = t/n}=e^{-\frac{t^2}{2n}} 
		\]
	\end{proof}
	
	\subsection{Discrepancy}
	\begin{thm}\label{thm: Discrepancy}
		Let $A_1\ldots A_m\subset [n]$, then there exists $f:[n]\to \cbk{\pm1}$ such that for all $i$,  
		\[
		\abs{\sum_{x\in A_i} f(x)}\le \sqrt{3n\log m}
		\]
	\end{thm}
	\begin{remark}
		[Spencer] When $m=n$,
		can reduce to $6\sqrt{n}$. \footnote{Six Standard Deviations Suffice}
 	\end{remark}
 	\begin{proof}
 		Let $f$ be chosen randomly. Then:
 		\[
 		\prob{\abs{\sum_{x\in A_i}f(x)} > \sqrt{3n\log m}} \le 2e^{\frac{3n\log m}{2|A_i|}}\le 2m^{-\frac{3}{2}}
 		\]
 		So
 		\[\prob{\exists i \text{ such that } \abs{\sum_{x\in A_i}f(x)} > \sqrt{3n\log m}} \le m\cdot 2m^{-\frac32}\nim{\text{Large enough } m} < 1
 		\]
 	\end{proof}
 	\begin{thm}\label{them: disc 2}
 		Let $G\sim \Gg(n,\frac{1}{2})$. Then with probability $1-o(1)$, for any $U\subset [n]$ of size $u$:
 		
 		\[
 		(\star) \qquad \abs{\abs{E(U)}-\frac{1}{2}{u\choose 2}}\le u^{\frac32}\sqrt{\log\rbk{\frac{en}{u}}}
 		\]
 	\end{thm}
 	\begin{proof}
 		\begin{align*}
 			&\prob{\text{The theorem Fails}} \le \sum_{u=1}^n{n\choose u}\cdot\prob{\neg (\star)} \le \sum_{u=1}^n\rbk{\frac{en}{u}}^u\cdot e^{-\frac{\rbk{2u^\frac32\sqrt{\log\frac{en}{u}}}^2}{2{u\choose 2}}} \le \\
 			&\sum_u
 		\end{align*}
 		\hl{Complete this}
 	\end{proof}
\subsection{Hidwiger's Conjecture}
Recall the $4$ color theorem: 
\begin{thm}[4-color theorem]\label{thm:4color}
	If $\chi(G)\ge 5$ then $G$ is nonplanar.
\end{thm}
\begin{thm}
	[Wagner]\label{thm: Wagner}
	$G$ is nonplanar iff $G$ contains $K_5$ or $K_{3,3}$ as a \emph{minor}\footnote{A subgraph obtained by deleting vertices, deleting edges or contracting edges}, and we denote $K_5,K_{3,3}\le G$
\end{thm}
Combining these results show that if $\chi(G)\ge 5$ then $G$ contains $K_5$ or $K_{3,3}$ as a minor. In fact - it can be shown that
\begin{thm}
	$\chi(G)\ge 5\Rightarrow K_5\le G$
\end{thm}
\begin{conjecture}
	$\chi(G)\ge t\Rightarrow K_t\le G$
\end{conjecture}
This is an open problem for $t\ge 7$.
\begin{conjecture}
	[Hajos]\label{conj:Hajos} If $\chi(G)\ge t$ then $G$ contains a \emph{partitioning}\footnote{Topological Minor} of $K_t$, and we denote $K_t\hookrightarrow G$.
\end{conjecture}
Funny thing - this conjecture is way, way off. The following theorem shows this.
\begin{thm}
	There exists a graph $G$ over $n$ vertices such that $\chi(G)\ge \frac{n}{2\log n}$, and $G$ does not contain $K_{10\sqrt{n}}$ as a topological minor.
\end{thm}

\begin{proof}
	Let $G\sim \Gg(n,1/2)$ and $K_{10\sqrt{n}}	\hookrightarrow G$. Then there exists $A\subset [n]$ with $|A| = 10\sqrt{n}$ that are the vertices of $K_{10\sqrt{n}}$ in the partitioning of its edges. Note that $|E(K_{10\sqrt{n}})| \approx 50n$. But at most $n$ edges of this embedding use vertices outside of $A$, so we need to show that w.h.p there is no such $A$ that contains $\ge 49n$ edges. This follows from Chernoff\autoref{them: disc 2}
\end{proof}
\section{Martingales}
\begin{yellowBox}
	\begin{defn}
		[Martingale] A sequence of random variables $z_0,z_1\ldots$ is called a \emph{Martingale} if $\Exp{\abs{z_i}}<\infty$ and
		\[
		\Exp{z_{i+1}\mid z_0\ldots z_i} = z_i
		\]
		Equivalently, 
	\[
	\Exp{z_{i+1} - z_i\mid z_0\ldots z_i} = 0
	\]
	\end{defn}
	\begin{remark}
		The second equality is much stronger than saying $\Exp{z_{i+1}-z_i} = 0$, as the equality in the definition is between \emph{Random Variables}.
	\end{remark}
\end{yellowBox}
\begin{example}
	Let $x_1\ldots x_n \nim{\text{ind.}}\sim \cbk{\pm1}$ and $z_i = \sum_{j=1}^i x_j$. Then:
	\[
	\Exp{z_{i+1}\mid z_0\ldots z_i} = \Exp{z_i + x_{i+1}\mid z_0\ldots z_i} = \Exp{z_i\mid z_0\ldots z_i} + \overbrace{\Exp{x_{i+1}\mid z_0\ldots z_i}}^0 = z_i
	\]
\end{example}
\begin{example}
	For any gambling strategy in a fair Casino, \hl{Complete this}
\end{example}
\begin{example}
	[Doob, exposure martingale] Let $x_1\ldots x_n\To \Omega$ be independent, and $f: \Omega ^n\to \RR$. Define
	\[
	z_i(x) \defeq \EE_{x_j\mid j>i}\sbk{f(x_1,\ldots x_i,x_{i+1},\ldots x_n}
	\]
	That is, in the $i$'th state we were given the values of $x_1\ldots x_i$, and $z_i$ gives his best guess to the value of $f(x)$ by taking expectation. Note that $z_0 = \Exp{f(x)}$ and $z_n = f(x)$.
	
	Consider $x_{i,j}\in \cbk{0,1}$ for any $1\le i < j \le n$ that encode a graph. Assume $f$ is some parameter of graph (say $\chi$). Then the exposure martingale is called the \emph{Edge Exposure Martingale}. 
	
	Consider $X_i = \cbk{j<i \mid j\sim_G i}$ for some underlying graph $G$. Then they can define (similarly) a martingale, that's called the \emph{Vertex Exposure Martingale}.
\end{example}

\begin{thm}
	[Azuma's inequality]\label{thm:Azuma} Let $z_i$ be a martingale such that $\abs{z_{i+1}-z_i} < c_{i+1}$\footnote{This assumption is called \emph{Bounded differences}}, and that $z_0$ is deterministic (that is - a constant RV). Then
	\[
	\prob{z_n-z_0 > t}\le e^{-\frac{t^2}{2\sum c_i^2}}
	\]
	That is - the place we "end the process" much further than where we started behaves like centralized RVs.
\end{thm}
\begin{remark}
	In the context of exposure martingales: Let $x_1,\ldots,x_n\in \Omega$ are independent, and $f$ is \emph{$C = (c_1,\ldots c_n)$-Lipschitz} in the sense that if $x,x'$ differ only in $x_i$, then $\abs{f(x)-f(x')}\le c_i$.  Then \[
	\prob{f(x) - \Exp{f(x)} > t}\le e^{-\frac{t^2}{2\sum c_i^2}}\]
\end{remark}
\begin{lemma}
\label{lemma: inverse Jensen}
	Assume $Y$ is a random variable with expectation $0$ and $|Y|\le c$. Then \[
	\Exp{e^Y}\le \frac{e^c + e^{-c}}{2}\le e^{-\frac{c^2}{2}}\] with equality iff $Y \overset{U}{\sim}\cbk{\pm c}$
\end{lemma}
\begin{proof}
	Consider the graph $e^Y$, say that the line connecting $e^c$ with $e^{-c}$ has the equation $a + by$, then by convexity - $e^Y \le a+bY$ so $\Exp{e^Y} \le \Exp{a + bY}\nim{\Exp{Y} = 0}= a$, and $a = \frac{e^c+e^{-c}}{2}$.
\end{proof}
\begin{proof}
	[Proof (Of \autoref{thm:Azuma}).]
	WLOG $z_0 = 0$. Now:
	\begin{align}
	 \Exp{e^{\lambda z_n}} &= \Exp{e^{\lambda(z_n-z_{n-1})}\cdot e^{\lambda z_{n-1}}} = \\
	& = \underset{z_1\ldots z_{n-1}}{\EE}\sbk{\Exp{e^{\lambda(z_n-z_{n-1})}\cdot e^{\lambda z_{n-1}}\mid z_1\ldots z_{n-1}}} = \\
	& = \underset{z_1\ldots z_{n-1}}{\EE}\sbk{\Exp{e^{\lambda(z_n-z_{n-1})}\mid z_1\ldots z_{n-1}}\cdot e^{\lambda z_{n-1}}}
	\end{align}
	Now, for every sample $z_1,\ldots z_{n-1}$,$\Exp{\lambda(z_n-z_{n-1})\mid z_1\ldots z_{n-1}} = 0$ by definition. Moreover, $\abs{\lambda(z_n-z{n-1})}<\abs{\lambda}c_n$. So by \autoref{lemma: inverse Jensen} taking $Y = \lambda(z_n - z_{n-1})$ \emph{conditioned by $z_1\ldots z_{n-1}$}, we have:
	\begin{align*}
		\underset{z_1\ldots z_{n-1}}{\EE}\sbk{\Exp{e^{\lambda(z_n-z_{n-1})}\mid z_1\ldots z_{n-1}}\cdot e^{\lambda z_{n-1}}} \le \underset{z_1\ldots z_{n-1}}{\EE}\sbk{e^{\frac{\lambda^2c_n^2}{2}}\cdot e^{\lambda z_{n-1}}}
	\end{align*}
	Continuing inductively, we get 
	\[
	\le\ldots \le e^{\lambda^2\rbk{\sum_i c_i^2}/2}
	\]
	And now
	\[
	\prob{z_n-z_0 > t} = \prob{e^{\lambda (z_n - z_0)} > e^{\lambda t}} \nim{\textrm{Markov}}\le \frac{e^{\lambda^2\rbk{\sum_i c_i^2}/2}}{e^{-\lambda t}}
	\]
	Optimizing $\lambda$ we get the desired bound.
\end{proof}
\begin{example}
	We throw $n$ balls into $n$ cells independently. Denote the number of empty cells by $L = \sum L_i$. Then $\Exp{L} = \sum_{i=1}^n \Exp{L_i} = n\cdot\rbk{1-\frac{1}{n}}^n \in \sbk{\frac{n-1}{e},\frac{n}{e}}$.
	\begin{claim}
		$\prob{\abs{L - \frac{n}{e}} > 1+t\sqrt{n}}\le e^{-\frac{t^2}{2}}$
	\end{claim}
	\begin{proof}
		Let $X_i\in [n]$ be the cell the $i$th ball went into. Note that $L(X_1,\ldots X_n)$ is $1$-Lispchitz, and the claim follows by Azuma.
	\end{proof}
\end{example}

\begin{example}
	Consider $\chi(G)$ with $G\sim \Gg(n,p)$.
	\begin{claim}
		For all $p$, let $X = \chi(G\sim \Gg(n,p))$. Then
		\[
		\prob{\abs{X - \Exp{X}} > t\sqrt{n}} < e^{-\frac{t^2}{2}}
		\]
	\end{claim}
	\begin{proof}
		We use the Vertex Exposure Martingale. $\chi$ is $1$-Lipschitz, and the claim follows by Azuma.
	\end{proof}
	In fact, this result can be strengthened. We do not prove this.
	\begin{claim}
		If $p = n^{-\alpha}$ with $\alpha > \frac{1}{2}$, then there exists $\mu = \mu(n,p)$ such that
		\[
		\prob{\mu \le \chi(\Gg(n,p)) \le \mu + 1}\nim{n\to\infty}\To 1
		\]
		This type of claim is called \emph{Two Point Concentration}
	\end{claim}
	\begin{claim}
		[Relaxation]
		If $p = n^{-\alpha}$ with $\alpha > \frac{5}{6}$, then there exists $\mu = \mu(n,p)$ such that
		\[
		\prob{\mu \le \chi\rbk{\Gg(n,p)} \le \mu + 3}\nim{n\to\infty}\To 1
		\]
	\end{claim}
	\begin{proof}
		Let $G\sim \Gg(n,p)$, and fix $\varepsilon$. Let $\mu$ be the maximal number such that $\prob{\chi(G) < \mu} < \varepsilon$. We show that $\prob{\chi(G) > \mu. +3} < \varepsilon$ and get the result. Let $Y$ be the minimal size of a subset $S\subset V(G)$ such that $G\setminus S$ is $\mu$-colorable. Note that $Y$ is $1$-Lipschitz with respect to vertex-exposure. Moreover
		\[
		e^{-\frac{\lambda^2}{2}}\varepsilon \le \prob{\chi(G) \ge \mu} = \prob{Y = 0} \nim{\mathrm{Azuma}}\le e^{-\frac{-\Exp{Y}^2}{2n}}
		\]
		and therefore $\Exp{Y} \le \lambda\sqrt{n}$. Now:
		\[
		\prob{Y >  2\lambda\sqrt{n}} = \prob{Y > \Exp{Y} + \lambda\sqrt{n}}\le e^{-\frac{\lambda^2}{2}} = \varepsilon
		\]
		We not prove this, but under the assumption on $p$ - any set of vertices of size $\le 2\lambda\sqrt{n}$ is $3$-colorable (because it's sparse), and the theorem follows.
	\end{proof}
\end{example}
\begin{example}
	Consider the Hamming Cube $H^n = \cbk{0,1}^n$ with Hamming distance $d_H(x,y)$.
	\begin{defn}
		Let $A\subset H^n$ and $t>0$. Define $A_t \defeq \cbk{x\mid d_H(x,A)\le t}$
	\end{defn}
	\begin{question}
	[Isoperimetric inequality] What is the minimal size of $A_t$ given $|A|$?
	\end{question}
	In Euclidian space this is asking the minimal perimeter body for a given volume? The answer is a sphere.
	\begin{thm}
		[Harper]\label{thm: Isoperimetric Harper} The optimal $A$ in $H^n$ is a ball.
	\end{thm}
	\begin{thm}
		If $|A| = \varepsilon \cdot 2^n$, then there exists $2\sqrt{2\log \frac{1}{\varepsilon}} = t>0$ such that $|A_{t\sqrt{n}}|>(1-\varepsilon)2^n$.
	\end{thm}
	\begin{proof}
		Let $x\in \cbk{0,1}^n$, and let $f(x) = d_H(x,A)$. Clearly $f$ is $1$-Lipschitz, and $\prob{f(x) =0} = \frac{|A|}{2^n} = \varepsilon$. But also,
		\begin{align*}
			\prob{f(x) = 0}\le \prob{f(x) - \Exp{f(x)}\le -\Exp{f(x)}}\nim{\text{Azuma for $-t$}}\le e^{-\frac{\Exp{f(x)}^2}{2n}}
		\end{align*}
		So $\Exp{f(x)}\le \sqrt{2\log\frac{1	}{\varepsilon}\cdot n}$, so now:
		\begin{align*}
			\frac{|\overline{A_{t\sqrt{n}}}|}{2^n} = \prob{f(x) > t\sqrt{n}} =\prob{f(x) - \Exp{f(x)} > \sqrt{2\log\frac{1	}{\varepsilon}\cdot n}}\nim{\text{Azuma + calculations}}\le \varepsilon
		\end{align*}
	\end{proof}
	\begin{thm}
		For any $A\subset H^n$, for any $t>0$, $\frac{|A|}{2^n}\cdot\frac{|\overline{A_{t\sqrt{n}}}|}{2^n}\le e^{-\frac{t^2}{4}}$
	\end{thm}
\end{example}
\section{Talagrand's Inequality}
	\subsection{The LIS problem}
	Talagrand's motivation was the \emph{Longest Increasing Subsequence} problem: Given $\pi\sim \Uni{S_n}$, what is the longest length of an increasing subsequence in $\pi$? Denote it $L$. It's relatively easy to show that w.h.p $L = \Theta(\sqrt{n})$. For the upper bound:
	\begin{align*}
		\Exp{\#\text{increasing subsequences of length }k} = {n\choose k}\cdot\frac{1}{k!}\leq \rbk{\frac{en}{k}}^k\cdot\rbk{\frac{e}{k}}^k = \rbk{\frac{e^2n}{k^2}}^k\nim{k = \Omega(\sqrt{n})}\To 0
	\end{align*}
	Now, take $x_i\sim \Uni{[0,1]}$ and order them increasingly. Then $L$ is $1$-Lipschitz, and then the concentration of measure is $\approx \sqrt{n}$. 
	\begin{yellowBox}
	\begin{defn}[Talagrand Distance]
		Let $A\subset \Omega^n$. The \emph{Talagrand Distance} between $A$ and $x\in \Omega^n$ is
		\[
		d_T(A,x) = \underset{\alpha\in \RR^n_+, \norm{\alpha}_2 = 1}{\max} \underset{y\in A}{\min} \sum_{i \text{ s.t }x_i\neq y_i}\alpha_i
		\]
	\end{defn}	
	\begin{remark}
		$d_T(A,x) \ge \frac{1}{\sqrt{n}}d_H(A,x)$ by taking $\alpha_i = 1/\sqrt{n}$
	\end{remark}
	\end{yellowBox}
	Therefore, take $B_t = \cbk{x\in \Omega^n \mid d_T(A,x)\ge t}$, then $B^H_t = \cbk{x\in \Omega^n\mid d_H(x,A) \ge \sqrt{n}t}$ is contained in $B_t$.

	\begin{thm}[Talagrand's Inequality]\label{thm: Talagrand}
		For any $A\subset \Omega^n, t>0$, $\prob{A}\cdot\prob{B} \le e^{-\frac{t^2}{4}}$
	\end{thm}
	\begin{yellowBox}
	\begin{defn}
		We say $f:\Omega^n\To \RR$ is \emph{$h$-certifiable}, where $h:\RR\to \RR$, if for all $x\in \Omega^n$ and $s\in \RR$, if $f(x)\ge s$ there exists $I\subset [n]$ of cardinality $h(s)$ such that
		\[
		\forall y\in \Omega^n\quad y\mid_I = x\mid_I \Rightarrow f(y)\ge s
		\]
		That is - there is a subset of indices of size $h(s)$ that implies $f(y)\ge s$.
	\end{defn}	
	\end{yellowBox}
	Let $f$ be $1-$Lipschitz and $h$ certifiable, define $A \defeq \cbk{x\mid f(x) \le r-t\sqrt{h(r)}}, B \defeq \cbk{x\mid f(x) \ge r}$.
\begin{claim}
	$B\subset B_t$
\end{claim}
\begin{proof}
	We need to show that for any $x$ for which $f(x)\ge r$, there exists $\alpha\in \RR^n_+$ such that for any $y\in A$, $t\le \sum_{i:x_i\neq y_i} \alpha_i $. Let $x\in B$, and let $I$ be the certificate to $f(x)\ge r$, and let $\alpha = \frac{1}{\sqrt{|I|}}\one_{I}$. For any $y\in A$, $y$ and $x$ differ in at least $t\sqrt{h(r)}$ coordinates of $I$ since $f$ is Lipschitz, therefore
	\[
	\sum_{i:x_i\ne y_i} \alpha_i \ge t\sqrt{h(r)}\ge t
	\]
\end{proof}
\begin{cor}
$\prob{A}\cdot\prob{B}	\le e^{-\frac{t^2}{4}}$
\end{cor}
\begin{cor}
If $L$ is the LIS of a random permutation, then	\[\prob{L\le 2\sqrt{n} - t\sqrt[4]{n}}\cdot \prob{L\ge 2\sqrt{n}}\le e^{-\frac{t^2}{4}}\]
\end{cor}

\subsection{Random Matrices Application}
The question is how concentrated the maximal eigenvalue of a random symmetric matrix $X\in M_{n\times n}([-1,1])$? $\lambda_1(X) = \max_{\norm{v}_2 = 1}v^\top X v$, and therefore $2$-Lipschitz.

\begin{claim}
	For any $m\in \RR$, $t>0$, $\prob{\lambda_1 \le m}\cdot \prob{\lambda_1 \ge m+t}\le e^{-\frac{t^2}{64}}$
\end{claim}
\begin{proof}
	Let $B$ be the event $\lambda_1 \ge m+t$, and $A$ the event $\lambda_1\le m$. We show that $B\subset B_{t/4}$ by showing for any $X$ such that $B$ holds, there exists $\alpha\in \RR_+^{{n\choose 2}+n}$ with $\norm{\alpha} = 1$ such that for any $Y$ with $\lambda_1(Y)\le m$ it holds that 
	$\sum_{i,j}\alpha_{i,j}\one_{X^i_j\ne Y^i_j} \ge t$: Let $v\in S^{n-1}$ such that $v^\top X v = \lambda_1(X) \ge m+t$. Note that now, $v^\top Yv \le m$, hence \[
	t\le v^\top (X-Y)v = \sum_{i,j} v_iv_j(x^i_j - y^i_j) \le \sum_{i,j} \abs{v_i}\cdot \abs{v_j}\cdot 2\one_{X^i_j\ne Y^i_j}\le 4\cdot\sum_{i,j}\alpha^i_j \one_{X^i_j\ne Y^i_j}
	\]
	By taking 
	\begin{align*}
		\alpha^i_j = \begin{cases}
			2\abs{v_i}^2 & i=j\\
			4\abs{v_i}\abs{v_j} & i\ne j
		\end{cases}
	\end{align*}
	so we get what we want by Talagrand.
\end{proof}
\subsection{Geometric Interpretation of Talagrand Distance}
Let $A\subset \Omega^n, x\in \Omega^n$, define
\[
U(A,x) = \cbk{s\in \cbk{0,1}^n\mid \exists y\in A\quad s_i = 1 \iff x_i\ne y_i }
\]
Then $d_T(A,x) = \max_{\alpha}\min_{y\in U(A,x)}\sum s_i\alpha_i$. This is minimizing a linear functional on some subset of the cube. Define $V(A,x) = \conv(U(A,x))$, then
\[
d_T(A,x) = \max_{\alpha} \min_{v\in V(A,x)} \sum v_i \alpha_i
\]
\begin{claim}
	$d_T(A,x) = \min_{v\in V(A,x)} \norm{v}$
\end{claim}
\begin{proof}
	Let $v$ be such minimizer. Let $\alpha = \frac{v}{\norm{v}}$. For any $u\in V(A,x)$ we have:
	\[
	\sum\alpha_i u_i \ge \sum \alpha_i v_i = \sum v_i^2/\norm{v} = \norm{v}
	\]
	So we get $\ge$.
	On the other hand, take any $\alpha$, in particular the one that maximizes the RHS. We need to show that there is some $s\in U(A,x)$ such that $\sum \alpha_i s_i \le \norm{v}$. We write $v = \sum_{s\in U(A,x)} \lambda_s s$ (a convex combination). Then:
	\[
	\norm{v}\ge \sum\alpha_i v_i = \sum_{s\in U(A,x)} \lambda_s\tbk{\alpha,s}
	\]
	And since the RHS is convex combination, there must be some $s$ for which the inner product is at most $\norm{v}$.
\end{proof}
\section{Harris Inequality (FKG ineq.)}
\begin{yellowBox}
\begin{defn}
	An event $A\subset \RR^n$ is called \emph{increasing} if for all $x,y$ such that for any $i$, $x_i\le y_i$:
	\[
	x\in A\Rightarrow y\in A
	\]
\end{defn}	
\end{yellowBox}
Let $x_1,\ldots x_n\in \RR$ independent random variables.
\begin{thm}
	[Harris ineq.] \label{thm: Harris} if $A,B$ are increasing events, then:
	\[
	\prob{A\cap B}\ge \prob{A}\prob{B}
	\]
	If $B$ is of positive probability, then:
	\[
	\prob{A\mid B}\ge \prob{A}
	\]
\end{thm}
\begin{proof}
	We show that for any $f,g:\RR^n\to \RR$ that are monotone (in each coordinate), then $\Exp{fg}\ge \Exp{f}\Exp{g}$, we proceed by induction on $n$. For $n=1$:
	\begin{align*}
		\Exp{f(x)g(x)}-\Exp{f(x)}\Exp{g(x)} = \frac12\underset{x,y\text{i.i.d}}{\EE}\sbk{(f(x)-f(y))(g(x)-g(y))}\nim{\text{same sign}}\ge 0
	\end{align*}
	Denote $x = (\overbrace{x_1,\ldots x_{n-1}}^{\tilde{x}},x_n)$. Now:
	\begin{align*}
		\EE\sbk{f(x)g(x)} = \EE_{x_n}\sbk{\EE_{\tilde{x}}f(\tilde{x},x_n)g(\tilde{x},x_n)}\nim{\text{induction}}\ge \EE_{x_n}\sbk{\EE_{\tilde{x}}[f(\tilde{x},x_n)]\EE_{\tilde{x}}[g(\tilde{x},x_n)]}= \star
	\end{align*}
	Note that $\EE_{\tilde{x}}[f(\tilde{x},x_n)]$ is an increasing function of $x_n$, so by induction hypothesis:
	\begin{align*}
		\star \ge \EE_{x_n}\sbk{\EE_{\tilde{x}}[f(\tilde{x},x_n)]}\EE_{x_n}\sbk{\EE_{\tilde{x}}[g(\tilde{x},x_n)]} = \Exp{f}\Exp{g}
	\end{align*}
\end{proof}
\begin{remark}
	If $f$ is increasing and $g$ is decreasing, it's easy to show that the opposite inequality holds, and if $f,g$ both decreasing, then the same ineq. holds.
\end{remark}
\begin{example}
	What is the probability that $G\sim \Gg(n,p)$ is triangle free? Denote $T_{i,j,k}$ the event "the triangle $i,j,k$ appears", then $\overline{T_{i,j,k}}$ are decreasing, hence:
	\begin{align*}
		\prob{\cap_{i,j,k}\overline{T_{i,j,k}}} \nim{FKG}\ge \prod_{i,j,k}\prob{\overline{T_{i,j,k}}} = (1-p^3)^{n\choose 3}\nim{p=o(1)}=e^{-(1+o(1))\frac{p^3n^3}{6}} = e^{-(1+o(1))\Exp{\#\triangle}}
	\end{align*}
\end{example}
\begin{example}
	Let $G\sim \Gg(n,\frac{1}{2})$. Then:
	 \[\prob{\cap_{v\in V}\deg(v)\ge\frac{n-1}{2}}\nim{FKG}\ge \prod_{v\in V}\prob{\deg(v)\ge \frac{n-1}{2}} = \rbk{\frac{1}{2}}^n\]
	 Surprisingly - the true answer is $(c+o(1))^n$ for $c \approx 0.61$.
\end{example}
\begin{example}
	Let $\Ff\subset 2^{[n]}$ such that all $A,B\in F$ intersect. Then the maximal size of $\Ff$ is $2^{n-1}$ (a nice question in extremal combinatorics). How about $\Gg$ such that no two sets cover $[n]$? same question - take complement. Cliteman asked how large can $\Hh$ be such that both properties hold? $\abs{\Hh} \le 2^{n-2}$ - take all sets that include $1$ but does not include $2$. Can be proven using FKG.
\end{example}

\chapter{The Poisson Paradigm}

Recall that a Poisson random variable with parameter $\lambda$, denoted $X\sim \Pois{\lambda}$, can be though of as a limit of $X_n\sim \Bin{n,p}$. Note that setting $\lambda = np$
\[
\prob{X_n = k} = {n\choose k}p^k(1-p)^{n-k} = (1+o(1))\frac{n^k}{k!}\cdot\rbk{\frac{c}{k}}^k\cdot e^{-\lambda} \nim{n\to \infty}\to  e^{-\lambda}\cdot\frac{\lambda^k}{k!}
\]
That is - a sum of (many) random variable, almost$^\TM$ that each occur with small probability is close to a Poisson Random Variable

\section{Janson Inequalities}
	Let $A_1,\ldots A_k$ an increasing sequence of events, denote $\mu = \Exp{\sum_i \one_{A_i}}$, and $\Delta = \sum_{i\sim j} \prob{A_i\cap A_j}$, where $i\sim j$ if $A_i,A_j$ are dependent.
	Recall the following:
	\begin{enumerate}
		\item If $X = \sum_i \one_{A_i}$, then $\mu = \Exp{X}$, and $Var(X)\le \mu + \Delta$, hence $\prob{X = 0} \le \frac{\mu +\Delta}{\mu^2}$
		\item (Harris) $\prob{X = 0} \ge \prod_i \prob{\overline{A_i}} = \prod{1-\prob{A_i}} \approx e^{-(1+o(1)) p}$
	\end{enumerate}
\begin{thm}
	[Janson first inequality]\label{thm: janson one}
	Under the same setting, \[\prob{X = 0} = \prob{\cap_{i} \overline{A_i}}\le e^{-\mu + \frac{\Delta}{2}}\]
\end{thm}
\begin{thm}
	[Janson second inequality]\label{thm: janson two}
	If $\mu \le \Delta$,
	 \[\prob{X = 0} = \prob{\cap_{i} \overline{A_i}}\le e^{-\frac{\mu^2}{2\Delta}}\]
\end{thm}
\begin{proof}
	First, we show that \ref{thm: janson one} implies \ref{thm: janson two}. For any $T\subset [k]$ we have:
	\[
		\prob{\cap_i\overline{A_i}}\le \prob{\cap_{i\in T}\overline{A_i}} \le e^{-\mu_T+\frac{\Delta_T}{2}}
	\]
	Now, choose $i\in T$ with probability $q$. Then 
	\[
		\Exp{-\mu_T + \frac{\Delta_T}{2}} = -q\cdot \mu + \frac{q^2\Delta}{2} = \frac{-\mu}{2\Delta}
	\]
	Taking $q = \frac{\mu}{\Delta} \le 1$ by assumption gives the last equality. So there must be some $T$ with $-\mu_T+\frac{\Delta_T}{2}\le \frac{-\mu}{2\Delta}$, ending this claim.
	\begin{lemma}
		$\prob{A_i\mid \overline{A_j}: j<i} \ge \prob{A_i} - \sum_{j<i, j\sim i} \prob{A_j\cap A_i}$
	\end{lemma}
	\begin{proof}
		Denote $B = \underset{j<i,j\sim i}{\cap}\overline{A_j}, C =\underset{j<i,j\not\sim i}{\cap}\overline{A_j}$. Then:
		
	\begin{align*}
		&\prob{A_i\mid BC} = \frac{\prob{A_iBC}}{\prob{BC}}\ge \frac{\prob{A_iBC}}{\prob{C}} = \prob{A_iB\mid C} = \prob{A_i\mid C} - \prob{A_i\overline{B}\mid C} = \\
		& \prob{A_i} - \prob{A_i\overline{B}\mid C}\nim{\star} \ge \prob{A_i} - \prob{A_i\overline{B}} = \prob{A_i} - \prob{A_i\cap \bigcup_{j\sim i, j<i}A_j} \nim{\text{Union Bound}}\ge \\
		& \prob{A_i} - \sum_{i<j,i\sim j}\prob{A_i\cap A_j}
	\end{align*}
	with $\star$ is since $A_i$ are increasing, $C$ is decreasing and $\overline{B}$ is increasing. Then by FKG, they are of negative correlation, so the inequality holds
	\end{proof}
	Now, denoting $p_i = \prob{A_i\mid \overline{A_j}: j<i}$ we have
		\begin{align*}
		\prob{\cap\overline{A_i}} = \prod(1-p_i) \le e^{\sum p_i} = e^{-\mu + \frac{\Delta}{2}}
	\end{align*}
	and we are done.
\end{proof}
\begin{remark}
	It always holds that $\prob{X = 0} \le e^{-\frac{\mu}{2}} + e^{-\frac{\mu^2}{2\Delta}}$. This is an exponential bound, much better than the polynomial bound obtained by second moment methods.
\end{remark}
\subsection{Triangles in $\Gg(n,p)$ - again!}
Let $G\sim \Gg(n,p)$, what is the probability that $G$ is triangle free? Set $A_{i,j,k}$ be the event $i,j,k$ are a triangle in $G$, hence $\mu = \frac{n^3p^3}{6}$ and $\Delta = \frac{n^4p^5}{2}$ (we've done this before). When is $\mu > \Delta$? When $p < \frac{1}{\sqrt{3n}}$. Hence
\begin{align*}
	\prob{\text{Triangle Free}} \le \begin{cases}
e^{-\mu(1+o(1))} &  p \ll \frac{1}{\sqrt{3n}}\\
e^{-cn^2p} & p \gg \frac{1}{\sqrt{3n}}
 \end{cases}
\end{align*}

We've already said that when $p\ll\frac{1}{n}$ then $\prob{\text{Triangle Free}}\to 1$, and when $p\gg\frac{1}{n}$ then $\prob{\text{Triangle Free}}\to 0$. What happens when $p = \Theta(1/n) = \frac{c}{n}$? In that case, $\mu \to \frac{c^3}{6}$, and $\Delta = \Oo(1/n)$. So
\[
e^{-\frac{c^3}{6}}\longleftarrow \rbk{1-\rbk{\frac{c}{n}}^3}^{\frac{n}{3}} \nim{\text{Harris}}\le \prob{\text{Triangle Free}} \nim{\text{Jansen}}\le e^{-\frac{c^3}{6} + \Oo(1/n)}
\]
\subsection{Chromatic number of $\Gg(n,\frac{1}{2})$}
We've already established that $\chi(G)\ge (1+o(1))\frac{n}{2\log n}$
\begin{lemma}
	WHP, any $S\subset [n]$ of size no less of $\frac{n}{\log^2 n}$ contains a coclique of size $(1+o(1))2\log n$.
\end{lemma}
\begin{proof}
\begin{align*}
	\prob{\text{no such $S$ exists}} = {n\choose \frac{n}{\log^2 n}}\cdot \prob{S\text{ does not include such coclique}}
\end{align*}	
and continue using Janson...
\end{proof}
\begin{example}[Using Jansen]
\begin{claim}
	Let $C>2$, if $p = \rbk{\frac{c\cdot \log n}{n^5	}}^{1/6}$, then w.h.p between any two vertices of $G(n,p)$ there is a path of length $6$.
\end{claim}
\begin{proof}
	Fix two vertices $a,b$, and let $X$ be the number of paths of length $6$ between $a,b$. Then:
	\begin{align*}
	& \mu = \Exp{X} = n^5(1+o(1)) p^6 = c\cdot \log n	\\
	& \Delta = \Oo\rbk{n^5 \sum_{v=1}^5n^{5-v}\cdot p^{12-v}} = \mu^2\Oo\rbk{\sum{v=1}^5 (np)^{-v}} = \mu^2\Oo\rbk{n^{-\frac{1}{6}}}
	\end{align*}
	So the probability that there is no such path, that is
	$\prob{X = 0}$, satisfies:
	\begin{align*}
		\prob{X = 0}\nim{\text{Chebishev}}\le \frac{1}{c\log n} + \Oo\rbk{n^{-\frac{1}{6}}}
	\end{align*}
	So $\prob{\text{there are $a,b$ with no such path}}\le {n\choose 2}\frac{1}{c\log n}$ - not helpful at all! But using Jansen, we have:
	\begin{align*}
	\prob{X = 0}\le e^{-c\log n + \Oo\rbk{\frac{\log^2n}{n^{1/6}}}}
	\end{align*}
	Which gives an informative bound.
\end{proof}
\end{example}
\chapter{Bits and Pieces}
\section{Entropy Method}
\begin{yellowBox}
\begin{defn}[Entropy]
Let $X$ be a (finitely supported) random variable. It's \emph{Entropy} is $\Hh(X)\defeq -\sum_{x\in X}p(x)\log_w(p(x))$	
\end{defn}	
\end{yellowBox}
Some facts about entropy:
\begin{thm}
\begin{enumerate}
	\item $\Hh(X)\le \log_2(|X|)$, with equality iff $X\sim\Uni{Supp(X)}$
	\item $\Hh(p) = \Hh(\Ber{p}) = -p\log p - (1-p)\log (1-p)$, and an important fact is that $${n\choose p\cdot n} \approx 2^{\Hh(p)\cdot n}$$
	\item $X\sim \Bin{n,\frac{1}{2}}$, then $\Hh(X) = \frac{1}{2}\log n + o(1)$ \footnote{With the intuition that since $\Bin{n,\frac{1}{2}}$ is incredibly concentrated at $\mu$, we only need to understand the std, so $\sqrt{n}$.}
	\item \textbf{Chain Rule:} $\Hh(X,Y) = \Hh(X) + \Hh(Y\mid X)$ with $\Hh(Y\mid X) \defeq = \EE_{x\sim X}\sbk{\Hh(Y\mid X = x)}$
	\item $\Hh(Y\mid X)\le \Hh (Y)$
	\item $\Hh(X_1,\ldots ,X_n)\le \sum_{i} \Hh(X_i\mid X_{<i}) \le \sum_{i} \Hh(X_i)$
\end{enumerate}
\end{thm}
\begin{proof}
	All properties are a corollary of Jensen and convexity etc.
\end{proof}

\subsection{Fake Coins}
Let $A\subset [n]$ be a collection of \emph{fake} coins, we can sample $S\subset [n]$ and get $|A\cap S|$. 
\begin{claim}
	If $S_1\ldots S_k$ satisfy that for any $A\ne A'$ there exists $i$ such that $|A\cap S_i|\ne |A' \cap S_i|$, then $k \ge (1+o(1))\frac{2n}{\log n}$. That is, we need at least $k$ samples to classify $A$ precisely.
\end{claim}
\begin{proof}
	Let $A$ be some random subset of $[n]$. Let $X_i = \abs{S_i\cap A}$. We claim that there is some bijection $(X_1\ldots X_k)\leftrightarrow 2^{[n]}$ (that is, they encode all subsets). $(X_1\ldots X_k)$ is supported by $2^n$ elements and is distributed uniformly on them. Hence:
	\[
	2 = \log(2^n) = \Hh(X_1\ldots X_k) \le \sum_{i}\Hh(X_i)\nim{X_i\sim \Bin{|S_i|,\frac{1}{2}}}\le \sum_i (1+o(1))\log|S_i|\le k\rbk{\frac{1}{2} + o(1)}\log n
	\]
\end{proof}
\subsection{Shearer Inequality}
\begin{thm}
	[Shearer] $2\Hh(X,Y,Z) \le \Hh(X,Y) + \Hh(X,Z) + \Hh(Y,Z)$
\end{thm}
\begin{proof}
	\begin{align*}
	\Hh(X,Y,Z) = \Hh(X) + \Hh(Y\mid X) = \Hh(Z\mid X,Y)	
	\end{align*}
	so
		\begin{align*}
	2\Hh(X,Y,Z) = \Hh(X) + \Hh(Y\mid X) = \Hh(Z\mid X) + \Hh(X) + \Hh(Y) + \Hh(Z\mid Y)	
	\end{align*}	
	As required
\end{proof}
\begin{cor}
Let $S\subset \RR^3$, then $Vol(S)^2\le 	Ar(S_{x,y})Ar(S_{x,z})Ar(S_{y,z})$ with $S_{x,z}$ is the shadow of $S$ on $x,z$ plane.
\end{cor}
\begin{proof}
	Take some finite $P \subset \RR^3$. We show $|P|^2 \le |P_{x,y}|\cdot|P_{x,z}|\cdot|P_{y,z}|$: Let $(X,Y,Z)$ be a random point in $P$. Then
	$\log(|P|) = \Hh(X,Y,Z)$ so
	 $$\log|P|^2 = 2\Hh(X,Y,Z) \le \Hh(X,Y) + \Hh(X,Z) + \Hh(Y,Z) \le \log|P_{x,y}| + \log|P_{x,z}| +\log|P_{y,z}|$$
	 Taking exponent on both sides finishes the proof. For the continuous case - take a tight enough lattice $\varepsilon\ZZ^3\cap S$ approximating $S$ 
\end{proof}
\begin{thm}
	[Shearer, generalized]\label{thm: Shearer General}
	Let $X_1\ldots X_n$ be random variables and $A_1,\ldots A_m\subset[n]$ such that any $i\in [n]$ belongs to at least $k$ of the $A_j$'s. Denote by $X_{A_j}$ the (multivariate) random variable comprised of all $X_i$ such that $i\in A_j$ .Then:
	\[
	\sum_{j=1}^m \Hh\rbk{X_{A_j}} \ge k\Hh(X)
	\]
\end{thm}
\begin{remark}
	The previous Shearer inequality is a private case of this - taking $A_1 = \cbk{1,2}, A_2 =\cbk{1,3}, A_3 = \cbk{2,3}$
\end{remark}
\begin{proof}
	\begin{align*}
		&\sum_{j=1}^m \Hh\rbk{A_{X_j}} = \sum_{j=1}^m\sum_{i\in A_j} \Hh\rbk{X_i\mid X_t\text{ such that }t<i, t\in A_j} \ge \\
		&\sum_{j=1}^m\sum_{i\in A_j} \Hh\rbk{X_i\mid X_t\text{ such that }t<i}  \ge \sum_{i=1}^n \Hh\rbk{X_i \mid X_{<i}}\cdot k
 = k \cdot \Hh(X)	\end{align*}
\end{proof}
\begin{thm}
	[Kruskal Katona, easy case]\label{thm: Kruskal Katona} Let $G$ be a graph over $e$ edges and $t$ triangles. Then $t\le \frac{(2e)^{\frac{3}{2}}}{6}$
\end{thm}
\begin{remark}
	The hard(er) case states that if $e = {x\choose 2}$ for some $x$, then $t\le {x\choose 3}$ which is the intuition from taking $e$ be the edges of a complete graph. Thinking of $e = \frac{x^2}{2}$ and ${x\choose 3}$ as $\frac{x^3}{6}$ gives the intuition to the theorem
\end{remark}
\begin{proof}
	Let $X_1,X_2,X_3$ be a vertices of a triangle. Then 
	\[
	\log_2(6t) = \Hh(X_1,X_2,X_3)\nim{\ref{thm: Shearer General}}\le\frac{1}{2}\rbk{\Hh(X_1,X_2) + \Hh(X_2,X_3) + \Hh(X_1,X_3)} = \star
	\]
	Note that $(X_1,X_2)$ is some distribution on the edges - so $\Hh(X_1,X_2)\le \log(2e)$. Then
	\[
	\log_2(6t) \le \star \le \frac{3}{2}\log(2e)
	\]
	Completing the proof.
\end{proof}
\subsection{Jeff Khan's Theorem}
\begin{thm}[Jeff Khan]\label{thm: Jeff Khan}
	Let $G$ be a bipartite $d$--regular graph over $n$ vertices with parts $A,B$. Denote by $\iota(G)$ the number of independent sets in $G$. Then 
	\[
	\iota(G) \le (2^{d+1}-1)^{\frac{n}{2d}}
	\]
\end{thm}
\begin{remark}
	Take $K_{d,d}$, then $\iota(K_{d,d}) = 2^{d+1}-1$ (since we counted the empty set twice), so the theorem is in fact "the best case is to take copies of $K_{d,d}$".
\end{remark}

\begin{proof}
	Let $X = (X_1\ldots X_n)$ be indicator vector of a randomly chosen independent set. Then 
	\[
	\log(\iota(G)) = \Hh(X) = \Hh\rbk{A_A} + \Hh(X_B\mid X_A)\nim{\ref{thm: Shearer General}}\le \frac{1}{d}\sum_{b\in B}\Hh(X_{N_b}) + \sum_{b\in B}\Hh(X_b\mid X_{N_b})
	\]
	So we need to show that for any $b$, $\Hh(X_{N_b}) + d\Hh(X_b\mid X_{N_b}) \le \log(2^{d+1}-1)$. Let $Y$ be sampling of $X_{N_b}$ and of $d$ independent copies of $(X_b\mid X_{N_b})$, $X_b^{(1)},X_b^{(2)},\ldots X_b^{(d)}$. Note that $Y$ is supported on $\iota(K_{d,d})$. Now, $\Hh(Y) = \Hh(X_{N_b}) + \sum_{j=1}^d \Hh(X_b^{(i)}\mid X_{N_b})$ since $X_b^{(i)}$ are independent. So we are done.
\end{proof}

\subsection{Latin Squares}
\begin{yellowBox}
\begin{defn}
	[LatinSquare] A Latin square is an $n\times n$ array in which every sign $i\in [n]$ appears exactly once in every row and every column.
\end{defn}	

\end{yellowBox}
\begin{question}
	How many Latin squares are there?
\end{question}
Denote the number of Latin squares over $n$ symbols by $L_n$. A guess would be \[L_n \approx \overbrace{n^{n^2}}^\text{write every symbols}\cdot\overbrace{\rbk{\frac{n!}{n^n}}^{2n}}^\text{cosntrains} \approx \rbk{\frac{n}{e^2}}^{n^2}\]
\begin{thm}
	${\displaystyle L_n \le \rbk{(1+o(1))\frac{n}{e^2}}^{n^2}}$
\end{thm}
\begin{proof}
	Let $X_{i,j}$ be a random Latin square. Then
	\begin{align*}
		&\log(L_n) = \Hh(X) \nim{\star}= \underset{\text{random order }<}{\EE}\sbk{\sum_{i,j}\Hh(X_{i,j}\mid X_{<{i,j}})} = \\
		&\sum_{i,j}\EE_{<}\sbk{\Hh(X_{i,j}\mid X_{<i,j})} = n^2\EE_{<}\sbk{\Hh(X_{1,1}\mid X_{<1,1})} = n^2 \EE_< \overbrace{\underset{L}{\EE}\sbk{\Hh(X_{1,1}\mid X_{<11} = L_{<11}}}^\text{Expose according to random Latin square} \le \\
		& \EE_< \EE_L \sbk{\log\overbrace{(N_{1,1}(L,<))}^\text{\#\text{of signs not taken in $L_{1,1}$}}} = \EE_L \EE_< \log(N_{1,1}(L,<)) = \heartsuit
	\end{align*}
	$\star $ - we sample $<$ by sampling $t_{i,j}\sim \Uni{[0,1]}$ i.i.d. Therefore \[
	\EE_< \log(N_{1,1}) = \int_0^1 \EE_{[0,1]^{n^2-1}}\log(N_{1,1}(t_{1,1})) dt_{1,1} \nim{\text{Jensen}}\le \int_0^1 \log \Exp{N_{1,1}(t_{1,1})}dt_{1,1} = \triangle
	\]
	Now
	\[
	\Exp{N_{1,1}(t_{1,1},<,L)} = 1 + (n-1)(1-t_{1,1})^2
	\]
	Hence 
	\[
	\triangle = \int_0^1 \log(n(1-t_{1,1})^2(1+o(1)))dt_{1,1} = \log(n) + -2 + o(1)
	\]
	We now have
	\[
	\heartsuit \le n^2(\log(n) - 2/\ln(2) + o(1))
	\]
So calculations work.
	\end{proof}
	\subsection{Bergman Inequality}
	\begin{thm}
		[Bergman Inequality]\label{thm: Bergman} Let $G$ be a bipartite graph over $L\sqcup R$ with $|L| = |R| = n$, and let $d_1,\ldots d_n$ be the sequence of degrees in $L$. Then the number of perfect matchings in $G$ satisfies
		\[
		PM(G)\le \prod_{i=1}^n(d_i!)^\frac{1}{d_i}
		\]
	\end{thm}
	\begin{remark}
		$PM(G)$ can be though of as the \emph{permanent} (determinant without signs) of the $n\times n$ bi-adjacency matrix of $G$.
	\end{remark}
	\begin{cor}
		The number of latin squares over $n$ symbols satisfies $L_n \le \rbk{(1+o(1)\frac{n}{e^2}}^{n^2}$
	\end{cor}
	\begin{proof}
		Think of an empty matrix $n\times n$ and choose some permutation to fix the symbol $1$. Then, the empty cells of the matrix are a bipartite graph (with parts "rows" and "columns" as the parts). Assigning the symbol $2$ is to find a perfect matching in this graph, and by Bergman there are $\prod_{i\in [n]}(n-1)^{1/(n-1)}$ matchings. This gives 
		\[
		L_n\le \prod_{d = n}^1(d!)^{\frac{n}{d}}
		\]
		and the bound is achieved by Sterling approximation.
	\end{proof}
	
	\begin{proof}
		[Proof (Bergman, \ref{thm: Bergman})] Let $X = (X_1,\ldots X_n)$ where $X_i$ is the neighbor of $i\in L$ in a random perfect matching. Then:
		\begin{align*}
			&\log(PM(G))=\Hh(X) = \EE_{<}\sum_{i=1}^n \Hh(X_i\mid X_{<i}) - \sum_{i=1}^n \EE_{<}\Hh(X_i\mid X_{<i}) = \\
			&\sum_{i=1}^n \EE_{<}\sbk{\underset{M\text{ matching}}{\EE}\Hh(X_i\mid X_{<i} = M_{<i})} \le \sum_{i=1}^n \EE_{<,M}\log(\overbrace{N_i(<,M)}^\heartsuit)  = \\
			&\sum_{i=1}^n \frac{1}{d_i}\sum_{j=1}^{d_i}\log(j)  = \sum_{i=1}^n \frac{1}{d_i}\log(d_i!) = \sum_{i=1}^n \log\rbk{(d_i!)^{\frac{1}{d_i}}}
		\end{align*}
		Where $\heartsuit$ is the number of available neighbors of $i$ when we reach it in the order, which is the support of $X_i$ conditioned by $X_{<i} = M_{<i}$. Note that $N_i(<,M)\sim \Uni{[d_i]}$: Consider $N(i)$; then any permutation affects $N_i(<,M)$ by asking "how many vertices in $N(i)$ were exposed before $i$?". Since $<$ is uniform, then the place where $i$ was exposed is uniform. Taking exponent on both sides gives the result
	\end{proof}
	\subsection{GCFS}
	\begin{thm}
		[Easy theorem] If $\Ff\subset 2^{[n]}$ is intersecting, then $|Ff|\le 2^{n-1}$
	\end{thm}
	\begin{quest}
		What is the maximal size of a family of graphs $\Gg\subset 2^{[n]\choose 2}$ such that any $G_1,G_2\in \Gg$ intersect in a triagel?
	\end{quest}
	\begin{thm}
		[Alice, Filmus, Fridgit, 2012] $|\Gg|\le 2^{{n\choose 2}-3}$
	\end{thm}
	\begin{thm}
		[Gram-Chang-Frenkel-Shearer]
		$|\Gg|\le 2^{{n\choose 2}-2}$
	\end{thm}
	\begin{proof}
		Assume $n \equiv 0\mod 2$. For any $S\subset [n]$ with $|S| = \frac{n}{2}$, define $E_S \defeq \cbk{e\in E\mid e\in S \vee e\in \overline{S}}$. Let $K$ be the number of $S$'s fir which a given edge belongs to $E_S$ Note that 
		\[
		{n\choose 2}K = {n\choose \frac{n}{2}}\cdot |E_S|
		\]
		Then:
		\begin{align*}
			\Hh(G\in \Gg)\nim{Shearer\; \ref{thm: Shearer General}} \le \frac{1}{K}\sum_{S\in {[n]\choose \frac{n}{2}}} \Hh(\overbrace{G\mid_{E_S} \mid G\in \Gg}^\text{Intersecting Family!})\le \frac{1}{K}{n\choose \frac{n}{2}}(|E_S|-1)
		\end{align*}
	\end{proof}
	\section{Phase Transition in Random Graphs}
	The question at hand is how do connected components look in random graphs.
	\begin{thm}
		[Erdo\"{o}s, R\'{e}nyi, '59] \label{thm: Erdos Renyi} Let $G\sim \Gg(n,p)$ with $p = \frac{c}{n}$ for some $c > 0$. Denote by $C_i$ the size of the $i$'th connected components ($C_1$ being the largest). Then with high probability:
		\begin{enumerate}
			\item If $c<1$ then $C_1 = O(\log(n))$.
			\item If $c=1$, not that important but $C_1 = \Theta(n^{2/3})$. In fact, for any fixed $j$, $C_j = \Theta(n^{2/3})$
			\item If $c>1$ then $C_1 = \Theta(n)$, we call it the \textbf{giant} components, and $C_2 = O(\log(n))$.
		\end{enumerate}
	\end{thm}
	\begin{remark}
		The intuition here is that $c = np \approx\Exp{\deg(v)}$.
	\end{remark}
	\begin{yellowBox}
	\begin{defn}
		A Galton-Watson poisson tree with parameter $c$ is a tree generated by taking a root $r$, and generating $\Pois{c}$ successors, and continuing with each of the generated vertices. Denote the tree by $GW(c)$.\\
		More formally, let $Z_1,Z_2\ldots \nim{i.i.d.}\sim \Pois{c}$. Denote $Y_0 = 1$, and $Y_t \defeq Y_{t-1} - 1 + Z_t$, that is $Y_t = 1 + \sum_{i=1}^t (Z_i-1)$. We think of $z_t$ as the children of the $t$'th vertex, and $Y_t$ as the bumber of vertices we've seen in time $t$. Let $T = \inf \cbk{t\mid Y_t = 0}\in \NN\cup \cbk{\infty}$. 
	\end{defn}	
	\end{yellowBox}
	\begin{thm}
		[Erdo\"{o}s, R\'{e}nyi reformulated] \label{thm: Erdos Renyi reformulated}
		We have:
		\begin{enumerate}
			\item If $c \le 1$ then $\prob{T<\infty}\To 1$
			\item If $c > 1$ then $\prob{T=\infty} = y > 0$ with $1-y = e^{-cy}$.
		\end{enumerate}
	\end{thm}

	\begin{proof}
		[First Proof, using BFS]
		Note that $\deg(v)\sim \Bin{n-1, p}\approx \Pois{c}$. Then running BFS on $G$ results in (sort of) $GW(c)$ - in the BFS process, once we've exposed enough vertices, the successors of a vertex does not distribute as $\Pois{c}$ since the binomial distribution is not with parameter $n-1$. \\
		\begin{enumerate}
			\item Assume $c<1$. Note that $Y_t > 0 \iff \sum_{i=1}^t (z_i-1) \ge 0$, but this is a sum of i.i.d random variables with expectation $c < 1$, so by the law of large numbers, $\prob{\sum_{i=1}^t (z_1-1) \ge t}\To 0$
			\item Now assume $c\ge 1$. Denote $y = \prob{T=\infty}$, then:
			\[
			1-y = \prob{T < \infty} = \sum_{i=1}^\infty \prob{z_i = i}(1-y)^i = e^{-cy}
			\]
			With the last equality by opening the definition of poisson distribution. Then if $c =1$ we are done (y=0), and if $c>1$ then we have $y>0$.
		\end{enumerate}
	\end{proof}
	We can strengthen (3) in \ref{thm: Erdos Renyi}:
	\begin{claim}
		If $c>1$, then $C_1 = (y+o(1))n$ for $y$ that satisfies $1-y = e^-{cy}$.
	\end{claim}
	\begin{remark}
		If $c = 1+\varepsilon$, then $y \approx 2\varepsilon$.
	\end{remark}
	\begin{thm}
		Let $c>0$, denote the size of $GW(c)$ by $T_c$. Then for any $k\in \NN$,
		\[
		\prob{T_c = k} = \lim_{n\to \infty}\prob{\abs{C_v(\Gg\rbk{n,\frac{c}{n}}} = k} =\frac{ e^{-ck}(ck)^{k-1}}{k!} \approx\frac{1}{\sqrt{2\pi}}k^{-\frac{3}{2}}\rbk{ce^{1-c}}^k
		\]
		for some $v\in V$.
	\end{thm}
	\begin{proof}
		Let $z_1,\ldots,z_k$ be a sequence of numbers such that if $Y_t\defeq 1 + \sum_{i=1}^t(z_i-1)$, then $Y_t\ge 0$ for any $t<k$, and $Y_k = 0$. Then 
		\begin{align*}
			& \prob{T_c = k} = \sum_{z} \prob{\forall t\in [k]\quad Z_i = z_i}	= \sum_{z} \prod_{i=1}^k\prob{\Pois{c} = z_i}	
		\end{align*}
		Now,
		\begin{align*}
			& \prob{\abs{C_v(\Gg\rbk{n,\frac{c}{n}}} = k} = \sum_z \prob{\text{(any $i$) the $i$'th vtx in the BFS starting from $v$ had $z_i$ new neighbors}} = \\
			&\sum_z\prod_{i=1}^k\prob{\Bin{n-1-\sum_{j<i}z_j,\frac{c}{n}} = z_i} = \sum_z \prod_{i=1}^k(1+o(1))\prob{\Pois{c} = z_i}
		\end{align*}
		Which shows the first equality. 
		Now, what is $\prob{\abs{C_v(G)} = k}$?
		\[
		\prob{\abs{C_v(G)} = k, \text{and not a tree}}\le {n\choose k-1}2^{k\choose 2}\rbk{\frac{c}{n}}^k\To 0
		\]
		\[
		\prob{\abs{C_v(G)} = k, \text{and a tree}}= {n\choose k-1}k^{k-1}\rbk{\frac{c}{n}}^{k-1}\cdot \rbk{1-\frac{c}{k}}^{k(n-k)} \To \frac{(ck)^{k-1}e^{-ck}}{k!}
		\]
	\end{proof}
	Let $c = 1-\varepsilon$, then by Taylor's approximation, $ce^{1-c}\approx e^{-\frac{\varepsilon^2}{2}}$. Now,
	\[
	\prob{T_C \ge k} =e^{-\frac{\varepsilon^2}{2}k(1+o(1))}
	\]
	We now have:
	\begin{claim}
		If $c = 1-\varepsilon$, then $C_1 = O\rbk{\frac{1}{\varepsilon^2}\log(n)}$
	\end{claim}
	\begin{proof} For any $k$ (no necessarily fixed!)
		$\prob{C_v\rbk{\Gg(n,\frac{c}{n}}\ge u}\le (1+o(1)) \prob{T_C \ge u}$. Take $u = K\frac{\log(n)}{\varepsilon^2}$ ane we are done.
	\end{proof}
	\appendix
\chapter{Extras}
\section{Crossing number}
Turan worked on this in WWII.

\begin{defn}
	[Crossing Number] For a graph $G$, the \emph{Crossing Number} $c(G)$ is the minimal amount of intersections between edges when drawn in $\RR^2$
\end{defn}
\begin{claim}
	For any $G$, $c(G) \ge e-(3v-6) \ge e-3v$
\end{claim}
\begin{proof}
	From Euler's Formula
\end{proof}
And now consider taking a random induced subgraph $G'$ we have that $\Exp{c(G')} = p^4c\ge p^2e - 3pv$, thus
	\[
	c\ge \frac{e}{p^2}-\frac{3v}{p^3}
	\]
optimizing $p$ we get $p = \frac{9v}{2e}$, so when $e > \frac{9v}{2}$, then $c \ge \alpha\cdot \frac{e^3}{v^2}$ Plugging into LHS (when $p<1$). Therefore when taking a dense subgraph $e\sim v^2$, then $c\gtrsim v^4$
\end{document}
